{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import glob\n",
    "\n",
    "d_struct = []\n",
    "\n",
    "\n",
    "for x in glob.glob(\"data/snippets_data/*\"):\n",
    "\n",
    "    with open(x, 'rb') as fp:\n",
    "        if 'pkl' in x:\n",
    "            adj = pickle.load(fp)\n",
    "            print(x)\n",
    "            print(type(adj))         # <class 'scipy.sparse.coo.coo_matrix'>\n",
    "            print(adj.shape)         # (예: (1000, 500))\n",
    "            # print(adj.nnz)           # non-zero 원소 수\n",
    "            \n",
    "        else:\n",
    "            print(x)\n",
    "            x = json.load(fp)\n",
    "            print(len(x))\n",
    "            \n",
    "glob.glob(\"data/snippets_data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/jail_breaker/dataset_pairs.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c09a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels = []\n",
    "\n",
    "for label in df.label.unique():\n",
    "    if label.startswith('TemplateJailbreak'):\n",
    "        clean_labels.append('TemplateJailbreak')\n",
    "    else:\n",
    "        clean_labels.append(label)\n",
    "\n",
    "clean_labels = np.array(clean_labels)\n",
    "unique_labels = np.unique(clean_labels)\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# 정수 인덱스 라벨로 변환\n",
    "numeric_labels = [label2idx[label] for label in clean_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70384aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pair = dict([(str(k), v) for k, v in zip(clean_labels, numeric_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numeric_label'] = df.label.apply(lambda x: numeric_pair['TemplateJailbreak'] if x.startswith('TemplateJailbreak') else numeric_pair[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90028",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/jail_breaker/labels.json', 'w') as fp:\n",
    "    json.dump(list(df.numeric_label), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c039b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af88b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로딩\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "docs = list(df.text)\n",
    "\n",
    "# 3. 문서별 엔티티 추출\n",
    "doc_entities = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    ents = set(ent.text.strip().lower() for ent in doc.ents if ent.text.strip())\n",
    "    doc_entities.append(ents)\n",
    "\n",
    "# 4. 전체 엔티티 목록 만들기\n",
    "all_ents = sorted(set(e for ents in doc_entities for e in ents))\n",
    "ent2idx = {ent: idx for idx, ent in enumerate(all_ents)}\n",
    "\n",
    "# 5. 희소 행렬 만들기\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, ents in enumerate(doc_entities):\n",
    "    for ent in ents:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(ent2idx[ent])\n",
    "        data.append(1)\n",
    "\n",
    "A_qe = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_ents)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker/adj_query2entity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qe, f)\n",
    "\n",
    "# (선택) 엔티티 인덱싱도 저장\n",
    "with open(\"data/jail_breaker/ent2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ent2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01626860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트 (이미 정의된 상태라고 가정)\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 단어 토큰 추출\n",
    "doc_words = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    words = set(token.text.lower() for token in doc if token.is_alpha and not token.is_stop)\n",
    "    doc_words.append(words)\n",
    "\n",
    "# 4. 전체 단어 목록 및 인덱싱\n",
    "all_words = sorted(set(w for words in doc_words for w in words))\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "# 5. 희소 행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, words in enumerate(doc_words):\n",
    "    for word in words:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(word2idx[word])\n",
    "        data.append(1)\n",
    "\n",
    "A_qw = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_words)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker/adj_query2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qw, f)\n",
    "\n",
    "# (선택) word2idx도 따로 저장\n",
    "with open(\"data/jail_breaker/word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 POS 태그 추출\n",
    "doc_tags = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    tags = set(token.pos_ for token in doc if token.is_alpha)\n",
    "    doc_tags.append(tags)\n",
    "\n",
    "# 4. 전체 태그 인덱싱\n",
    "all_tags = sorted(set(t for tags in doc_tags for t in tags))\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "# 5. 희소행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, tags in enumerate(doc_tags):\n",
    "    for tag in tags:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(tag2idx[tag])\n",
    "        data.append(1)\n",
    "\n",
    "A_qt = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_tags)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker/adj_query2tag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qt, f)\n",
    "\n",
    "# (선택) tag2idx 저장\n",
    "with open(\"data/jail_breaker/tag2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tag2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "# 1. spaCy 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 단어 추출\n",
    "doc_words = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    words = set(token.text.lower() for token in doc if token.is_alpha and not token.is_stop)\n",
    "    doc_words.append(words)\n",
    "\n",
    "# 4. 단어 인덱싱\n",
    "all_words = sorted(set(w for words in doc_words for w in words))\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "# 5. 단어–단어 쌍 연결 (같은 문서에 등장한 모든 단어쌍)\n",
    "edges = set()\n",
    "for words in doc_words:\n",
    "    idxs = [word2idx[w] for w in words]\n",
    "    for i, j in combinations(idxs, 2):\n",
    "        a, b = sorted((i, j))\n",
    "        edges.add((a, b))\n",
    "\n",
    "# 6. 행렬 구성 (symmetric, binary)\n",
    "rows, cols, data = [], [], []\n",
    "for i, j in edges:\n",
    "    rows.append(i)\n",
    "    cols.append(j)\n",
    "    data.append(1)\n",
    "    # 대칭 추가\n",
    "    rows.append(j)\n",
    "    cols.append(i)\n",
    "    data.append(1)\n",
    "\n",
    "A_word = coo_matrix((data, (rows, cols)), shape=(len(all_words), len(all_words)))\n",
    "\n",
    "# 7. 저장\n",
    "with open(\"data/jail_breaker/adj_word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_word, f)\n",
    "\n",
    "with open(\"data/jail_breaker/word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로딩\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 POS 태그 수집\n",
    "doc_tags = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    tags = set(token.pos_ for token in doc if token.is_alpha)\n",
    "    doc_tags.append(tags)\n",
    "\n",
    "# 4. 태그 인덱스화\n",
    "all_tags = sorted(set(t for tags in doc_tags for t in tags))\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "# 5. 태그–태그 연결 (동일 문서에 등장한 쌍)\n",
    "edges = set()\n",
    "for tags in doc_tags:\n",
    "    idxs = [tag2idx[t] for t in tags]\n",
    "    for i, j in combinations(idxs, 2):\n",
    "        a, b = sorted((i, j))\n",
    "        edges.add((a, b))\n",
    "\n",
    "# 6. 행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for i, j in edges:\n",
    "    rows.append(i)\n",
    "    cols.append(j)\n",
    "    data.append(1)\n",
    "    rows.append(j)\n",
    "    cols.append(i)\n",
    "    data.append(1)\n",
    "\n",
    "A_tag = coo_matrix((data, (rows, cols)), shape=(len(all_tags), len(all_tags)))\n",
    "\n",
    "# 7. 저장\n",
    "# os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "with open(\"data/jail_breaker/adj_tag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_tag, f)\n",
    "\n",
    "with open(\"data/jail_breaker/tag2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tag2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. 엔티티 목록 로딩\n",
    "with open(\"data/jail_breaker/word2idx.pkl\", \"rb\") as f:\n",
    "    ent2idx = pickle.load(f)\n",
    "\n",
    "# 2. 임베딩 차원 설정\n",
    "dim = 300  # 논문 따라가거나 128, 256 등도 가능\n",
    "\n",
    "# 3. 랜덤 벡터 생성\n",
    "np.random.seed(42)\n",
    "entity_emb = np.random.randn(len(ent2idx), dim).astype(np.float32)\n",
    "\n",
    "# 4. 저장\n",
    "os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "with open(\"data/jail_breaker/entity_emb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261be8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. 단어 목록 로딩\n",
    "with open(\"data/jail_breaker/word2idx.pkl\", \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "# 2. 임베딩 차원\n",
    "dim = 300  # 통일성 있게 entity랑 맞추는 걸 추천\n",
    "\n",
    "# 3. 랜덤 임베딩 생성\n",
    "np.random.seed(42)\n",
    "word_emb = np.random.randn(len(word2idx), dim).astype(np.float32)\n",
    "\n",
    "# 4. 저장\n",
    "os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "with open(\"data/jail_breaker/word_emb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# 전체 문서 수\n",
    "num_docs = len(docs)\n",
    "\n",
    "# 전체 인덱스\n",
    "all_indices = list(range(num_docs))\n",
    "\n",
    "# train/test 분할 (ex: 80/20)\n",
    "train_idx, test_idx = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# 저장 경로\n",
    "os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "# 저장\n",
    "with open(\"data/jail_breaker/train_idx.json\", \"w\") as f:\n",
    "    json.dump(train_idx, f)\n",
    "\n",
    "with open(\"data/jail_breaker/test_idx.json\", \"w\") as f:\n",
    "    json.dump(test_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e48395",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --dataset jail_breaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47beefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82612a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taehun_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
