{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import glob\n",
    "\n",
    "# originalMultiLabelData\n",
    "for x in glob.glob(\"data/snippets_data/*\"):\n",
    "\n",
    "    with open(x, 'rb') as fp:\n",
    "        if 'pkl' in x:\n",
    "            adj = pickle.load(fp)\n",
    "            print(x)\n",
    "            print(type(adj))         # <class 'scipy.sparse.coo.coo_matrix'>\n",
    "            print(adj.shape)         # (예: (1000, 500))\n",
    "            # print(adj.nnz)           # non-zero 원소 수\n",
    "            \n",
    "        else:\n",
    "            print(x)\n",
    "            x = json.load(fp)\n",
    "            print(len(x))\n",
    "            \n",
    "glob.glob(\"data/snippets_data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/jail_breaker_data/dataset_pairs.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c09a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels = []\n",
    "\n",
    "for label in df.label.unique():\n",
    "    if label.startswith('TemplateJailbreak'):\n",
    "        clean_labels.append('TemplateJailbreak')\n",
    "    else:\n",
    "        clean_labels.append(label)\n",
    "\n",
    "clean_labels = np.array(clean_labels)\n",
    "unique_labels = np.unique(clean_labels)\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# 정수 인덱스 라벨로 변환\n",
    "numeric_labels = [label2idx[label] for label in clean_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70384aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pair = dict([(str(k), v) for k, v in zip(clean_labels, numeric_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numeric_label'] = df.label.apply(lambda x: numeric_pair['TemplateJailbreak'] if x.startswith('TemplateJailbreak') else numeric_pair[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df.text)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/jail_breaker/labels.json', 'w') as fp:\n",
    "#     json.dump(list(df.numeric_label), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf453dd",
   "metadata": {},
   "source": [
    "##### Entity Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로딩\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 엔티티 추출\n",
    "doc_entities = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    ents = set(ent.text.strip().lower() for ent in doc.ents if ent.text.strip())\n",
    "    doc_entities.append(ents)\n",
    "\n",
    "# 4. 전체 엔티티 목록 만들기\n",
    "all_ents = sorted(set(e for ents in doc_entities for e in ents))\n",
    "ent2idx = {ent: idx for idx, ent in enumerate(all_ents)}\n",
    "\n",
    "# 5. 희소 행렬 만들기\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, ents in enumerate(doc_entities):\n",
    "    for ent in ents:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(ent2idx[ent])\n",
    "        data.append(1)\n",
    "\n",
    "A_qe = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_ents)))\n",
    "\n",
    "# 6. 희소행렬 저장\n",
    "os.makedirs(\"data/jail_breaker_data\", exist_ok=True)\n",
    "with open(\"data/jail_breaker_data/adj_query2entity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qe, f)\n",
    "\n",
    "# 7. 엔티티 임베딩 바로 생성 및 저장\n",
    "dim = 300\n",
    "np.random.seed(42)\n",
    "entity_emb = np.random.randn(len(all_ents), dim).astype(np.float32)\n",
    "with open(\"data/jail_breaker_data/entity_emb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_emb, f)\n",
    "\n",
    "print(f\"adj_query2entity shape: {A_qe.shape}\")\n",
    "print(f\"entity_emb shape: {entity_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af88b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from scipy.sparse import coo_matrix\n",
    "# import pickle\n",
    "\n",
    "# # 1. spaCy 모델 로딩\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # 2. 문서 리스트\n",
    "# docs = docs\n",
    "\n",
    "# # 3. 문서별 엔티티 추출\n",
    "# doc_entities = []\n",
    "# for text in docs:\n",
    "#     doc = nlp(text)\n",
    "#     ents = set(ent.text.strip().lower() for ent in doc.ents if ent.text.strip())\n",
    "#     doc_entities.append(ents)\n",
    "\n",
    "# # 4. 전체 엔티티 목록 만들기\n",
    "# all_ents = sorted(set(e for ents in doc_entities for e in ents))\n",
    "# ent2idx = {ent: idx for idx, ent in enumerate(all_ents)}\n",
    "\n",
    "# # 5. 희소 행렬 만들기\n",
    "# rows, cols, data = [], [], []\n",
    "# for doc_idx, ents in enumerate(doc_entities):\n",
    "#     for ent in ents:\n",
    "#         rows.append(doc_idx)\n",
    "#         cols.append(ent2idx[ent])\n",
    "#         data.append(1)\n",
    "\n",
    "# A_qe = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_ents)))\n",
    "\n",
    "# # 6. 저장\n",
    "# with open(\"data/jail_breaker/adj_query2entity.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(A_qe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013d82c",
   "metadata": {},
   "source": [
    "##### Word Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01626860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트 (이미 정의된 상태라고 가정)\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 단어 토큰 추출\n",
    "doc_words = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    words = set(token.text.lower() for token in doc if token.is_alpha and not token.is_stop)\n",
    "    doc_words.append(words)\n",
    "\n",
    "# 4. 전체 단어 목록 및 인덱싱\n",
    "all_words = sorted(set(w for words in doc_words for w in words))\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "# 5. 희소 행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, words in enumerate(doc_words):\n",
    "    for word in words:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(word2idx[word])\n",
    "        data.append(1)\n",
    "\n",
    "A_qw = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_words)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker_data/adj_query2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. docs에서 단어 집합 추출 (중복 제거)\n",
    "words = set()\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            words.add(token.text.lower())\n",
    "words = sorted(words)\n",
    "\n",
    "# 3. 임베딩 차원\n",
    "dim = 300\n",
    "\n",
    "# 4. 랜덤 임베딩 생성\n",
    "np.random.seed(42)\n",
    "word_emb = np.random.randn(len(words), dim).astype(np.float32)\n",
    "\n",
    "# 5. 저장\n",
    "with open(\"data/jail_breaker_data/word_emb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_emb, f)\n",
    "\n",
    "print(f\"word_emb shape: {word_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b4903",
   "metadata": {},
   "source": [
    "#### Tag Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 POS 태그 추출\n",
    "doc_tags = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    tags = set(token.pos_ for token in doc if token.is_alpha)\n",
    "    doc_tags.append(tags)\n",
    "\n",
    "# 4. 전체 태그 인덱싱\n",
    "all_tags = sorted(set(t for tags in doc_tags for t in tags))\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "# 5. 희소행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, tags in enumerate(doc_tags):\n",
    "    for tag in tags:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(tag2idx[tag])\n",
    "        data.append(1)\n",
    "\n",
    "A_qt = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_tags)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker/adj_query2tag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ff5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로딩\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 POS 태그 수집\n",
    "doc_tags = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    tags = set(token.pos_ for token in doc if token.is_alpha)\n",
    "    doc_tags.append(tags)\n",
    "\n",
    "# 4. 태그 인덱스화\n",
    "all_tags = sorted(set(t for tags in doc_tags for t in tags))\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "# 5. 태그–태그 연결 (동일 문서에 등장한 쌍)\n",
    "edges = set()\n",
    "for tags in doc_tags:\n",
    "    idxs = [tag2idx[t] for t in tags]\n",
    "    for i, j in combinations(idxs, 2):\n",
    "        a, b = sorted((i, j))\n",
    "        edges.add((a, b))\n",
    "\n",
    "# 6. 행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for i, j in edges:\n",
    "    rows.append(i)\n",
    "    cols.append(j)\n",
    "    data.append(1)\n",
    "    rows.append(j)\n",
    "    cols.append(i)\n",
    "    data.append(1)\n",
    "\n",
    "A_tag = coo_matrix((data, (rows, cols)), shape=(len(all_tags), len(all_tags)))\n",
    "\n",
    "# 7. 저장\n",
    "# os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "with open(\"data/jail_breaker/adj_tag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_tag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# # 1. spaCy 모델 로드\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # 2. docs에서 단어 집합 추출 (중복 제거)\n",
    "# words = set()\n",
    "# for text in docs:\n",
    "#     doc = nlp(text)\n",
    "#     for token in doc:\n",
    "#         if token.is_alpha and not token.is_stop:\n",
    "#             words.add(token.text.lower())\n",
    "# words = sorted(words)\n",
    "\n",
    "# # 3. 임베딩 차원\n",
    "# dim = 300\n",
    "\n",
    "# # 4. 랜덤 임베딩 생성\n",
    "# np.random.seed(42)\n",
    "# word_emb = np.random.randn(len(words), dim).astype(np.float32)\n",
    "\n",
    "# # 5. 저장\n",
    "# os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "# with open(\"data/jail_breaker/word_emb.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(word_emb, f)\n",
    "\n",
    "# print(f\"word_emb shape: {word_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc55b3",
   "metadata": {},
   "source": [
    "##### Split Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# 전체 문서 수\n",
    "num_docs = len(docs)\n",
    "\n",
    "# 전체 인덱스\n",
    "all_indices = list(range(num_docs))\n",
    "\n",
    "# train/test 분할 (ex: 80/20)\n",
    "train_idx, test_idx = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# 저장 경로\n",
    "os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "# 저장\n",
    "with open(\"data/jail_breaker/train_idx.json\", \"w\") as f:\n",
    "    json.dump(train_idx, f)\n",
    "\n",
    "with open(\"data/jail_breaker/test_idx.json\", \"w\") as f:\n",
    "    json.dump(test_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f28beb",
   "metadata": {},
   "source": [
    "##### Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e662d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Shape Check\n",
    "for x in glob.glob('data/jail_breaker_data/*'):\n",
    "    with open(x, 'rb') as fp:\n",
    "        if 'pkl' in x:\n",
    "            adj = pickle.load(fp)\n",
    "            print(x)\n",
    "            print(type(adj))         # <class 'scipy.sparse.coo.coo_matrix'>\n",
    "            print(adj.shape)         # (예: (1000, 500))\n",
    "            # print(adj.nnz)           # non-zero 원소 수\n",
    "            \n",
    "        elif 'json' in x:\n",
    "            print(x)\n",
    "            x = json.load(fp)\n",
    "            print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e48395",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/bighillanal/SimSTCFork/train_log.md', 'w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "!python train.py --dataset jail_breaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47beefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taehun_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
