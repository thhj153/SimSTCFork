{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import glob\n",
    "\n",
    "# originalMultiLabelData\n",
    "for x in glob.glob(\"data/snippets_data/*\"):\n",
    "\n",
    "    with open(x, 'rb') as fp:\n",
    "        if 'pkl' in x:\n",
    "            adj = pickle.load(fp)\n",
    "            print(x)\n",
    "            print(type(adj))         # <class 'scipy.sparse.coo.coo_matrix'>\n",
    "            print(adj.shape)         # (예: (1000, 500))\n",
    "            # print(adj.nnz)           # non-zero 원소 수\n",
    "            \n",
    "        else:\n",
    "            print(x)\n",
    "            x = json.load(fp)\n",
    "            print(len(x))\n",
    "            \n",
    "glob.glob(\"data/snippets_data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/jail_breaker_data/dataset_pairs.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c09a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels = []\n",
    "\n",
    "for label in df.label.unique():\n",
    "    if label.startswith('TemplateJailbreak'):\n",
    "        clean_labels.append('TemplateJailbreak')\n",
    "    else:\n",
    "        clean_labels.append(label)\n",
    "\n",
    "clean_labels = np.array(clean_labels)\n",
    "unique_labels = np.unique(clean_labels)\n",
    "label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# 정수 인덱스 라벨로 변환\n",
    "numeric_labels = [label2idx[label] for label in clean_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70384aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pair = dict([(str(k), v) for k, v in zip(clean_labels, numeric_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numeric_label'] = df.label.apply(lambda x: numeric_pair['TemplateJailbreak'] if x.startswith('TemplateJailbreak') else numeric_pair[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df.text)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/jail_breaker/labels.json', 'w') as fp:\n",
    "#     json.dump(list(df.numeric_label), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf453dd",
   "metadata": {},
   "source": [
    "##### Entity Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로딩\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 엔티티 추출\n",
    "doc_entities = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    ents = set(ent.text.strip().lower() for ent in doc.ents if ent.text.strip())\n",
    "    doc_entities.append(ents)\n",
    "\n",
    "# 4. 전체 엔티티 목록 만들기\n",
    "all_ents = sorted(set(e for ents in doc_entities for e in ents))\n",
    "ent2idx = {ent: idx for idx, ent in enumerate(all_ents)}\n",
    "\n",
    "# 5. 희소 행렬 만들기\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, ents in enumerate(doc_entities):\n",
    "    for ent in ents:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(ent2idx[ent])\n",
    "        data.append(1)\n",
    "\n",
    "A_qe = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_ents)))\n",
    "\n",
    "# 6. 희소행렬 저장\n",
    "os.makedirs(\"data/jail_breaker_data\", exist_ok=True)\n",
    "with open(\"data/jail_breaker_data/adj_query2entity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qe, f)\n",
    "\n",
    "# 7. 엔티티 임베딩 바로 생성 및 저장\n",
    "dim = 300\n",
    "np.random.seed(42)\n",
    "entity_emb = np.random.randn(len(all_ents), dim).astype(np.float32)\n",
    "with open(\"data/jail_breaker_data/entity_emb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_emb, f)\n",
    "\n",
    "print(f\"adj_query2entity shape: {A_qe.shape}\")\n",
    "print(f\"entity_emb shape: {entity_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af88b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from scipy.sparse import coo_matrix\n",
    "# import pickle\n",
    "\n",
    "# # 1. spaCy 모델 로딩\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # 2. 문서 리스트\n",
    "# docs = docs\n",
    "\n",
    "# # 3. 문서별 엔티티 추출\n",
    "# doc_entities = []\n",
    "# for text in docs:\n",
    "#     doc = nlp(text)\n",
    "#     ents = set(ent.text.strip().lower() for ent in doc.ents if ent.text.strip())\n",
    "#     doc_entities.append(ents)\n",
    "\n",
    "# # 4. 전체 엔티티 목록 만들기\n",
    "# all_ents = sorted(set(e for ents in doc_entities for e in ents))\n",
    "# ent2idx = {ent: idx for idx, ent in enumerate(all_ents)}\n",
    "\n",
    "# # 5. 희소 행렬 만들기\n",
    "# rows, cols, data = [], [], []\n",
    "# for doc_idx, ents in enumerate(doc_entities):\n",
    "#     for ent in ents:\n",
    "#         rows.append(doc_idx)\n",
    "#         cols.append(ent2idx[ent])\n",
    "#         data.append(1)\n",
    "\n",
    "# A_qe = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_ents)))\n",
    "\n",
    "# # 6. 저장\n",
    "# with open(\"data/jail_breaker/adj_query2entity.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(A_qe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013d82c",
   "metadata": {},
   "source": [
    "##### Word Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01626860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트 (이미 정의된 상태라고 가정)\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 단어 토큰 추출\n",
    "doc_words = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    words = set(token.text.lower() for token in doc if token.is_alpha and not token.is_stop)\n",
    "    doc_words.append(words)\n",
    "\n",
    "# 4. 전체 단어 목록 및 인덱싱\n",
    "all_words = sorted(set(w for words in doc_words for w in words))\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "# 5. 희소 행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, words in enumerate(doc_words):\n",
    "    for word in words:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(word2idx[word])\n",
    "        data.append(1)\n",
    "\n",
    "A_qw = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_words)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker_data/adj_query2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. docs에서 단어 집합 추출 (중복 제거)\n",
    "words = set()\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            words.add(token.text.lower())\n",
    "words = sorted(words)\n",
    "\n",
    "# 3. 임베딩 차원\n",
    "dim = 300\n",
    "\n",
    "# 4. 랜덤 임베딩 생성\n",
    "np.random.seed(42)\n",
    "word_emb = np.random.randn(len(words), dim).astype(np.float32)\n",
    "\n",
    "# 5. 저장\n",
    "with open(\"data/jail_breaker_data/word_emb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_emb, f)\n",
    "\n",
    "print(f\"word_emb shape: {word_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b4903",
   "metadata": {},
   "source": [
    "#### Tag Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "\n",
    "# 1. spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 POS 태그 추출\n",
    "doc_tags = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    tags = set(token.pos_ for token in doc if token.is_alpha)\n",
    "    doc_tags.append(tags)\n",
    "\n",
    "# 4. 전체 태그 인덱싱\n",
    "all_tags = sorted(set(t for tags in doc_tags for t in tags))\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "# 5. 희소행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for doc_idx, tags in enumerate(doc_tags):\n",
    "    for tag in tags:\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(tag2idx[tag])\n",
    "        data.append(1)\n",
    "\n",
    "A_qt = coo_matrix((data, (rows, cols)), shape=(len(docs), len(all_tags)))\n",
    "\n",
    "# 6. 저장\n",
    "with open(\"data/jail_breaker/adj_query2tag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_qt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ff5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "# 1. spaCy 모델 로딩\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 문서 리스트\n",
    "# docs = [...]\n",
    "\n",
    "# 3. 문서별 POS 태그 수집\n",
    "doc_tags = []\n",
    "for text in docs:\n",
    "    doc = nlp(text)\n",
    "    tags = set(token.pos_ for token in doc if token.is_alpha)\n",
    "    doc_tags.append(tags)\n",
    "\n",
    "# 4. 태그 인덱스화\n",
    "all_tags = sorted(set(t for tags in doc_tags for t in tags))\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "# 5. 태그–태그 연결 (동일 문서에 등장한 쌍)\n",
    "edges = set()\n",
    "for tags in doc_tags:\n",
    "    idxs = [tag2idx[t] for t in tags]\n",
    "    for i, j in combinations(idxs, 2):\n",
    "        a, b = sorted((i, j))\n",
    "        edges.add((a, b))\n",
    "\n",
    "# 6. 행렬 구성\n",
    "rows, cols, data = [], [], []\n",
    "for i, j in edges:\n",
    "    rows.append(i)\n",
    "    cols.append(j)\n",
    "    data.append(1)\n",
    "    rows.append(j)\n",
    "    cols.append(i)\n",
    "    data.append(1)\n",
    "\n",
    "A_tag = coo_matrix((data, (rows, cols)), shape=(len(all_tags), len(all_tags)))\n",
    "\n",
    "# 7. 저장\n",
    "# os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "with open(\"data/jail_breaker/adj_tag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(A_tag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# # 1. spaCy 모델 로드\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # 2. docs에서 단어 집합 추출 (중복 제거)\n",
    "# words = set()\n",
    "# for text in docs:\n",
    "#     doc = nlp(text)\n",
    "#     for token in doc:\n",
    "#         if token.is_alpha and not token.is_stop:\n",
    "#             words.add(token.text.lower())\n",
    "# words = sorted(words)\n",
    "\n",
    "# # 3. 임베딩 차원\n",
    "# dim = 300\n",
    "\n",
    "# # 4. 랜덤 임베딩 생성\n",
    "# np.random.seed(42)\n",
    "# word_emb = np.random.randn(len(words), dim).astype(np.float32)\n",
    "\n",
    "# # 5. 저장\n",
    "# os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "# with open(\"data/jail_breaker/word_emb.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(word_emb, f)\n",
    "\n",
    "# print(f\"word_emb shape: {word_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc55b3",
   "metadata": {},
   "source": [
    "##### Split Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# 전체 문서 수\n",
    "num_docs = len(docs)\n",
    "\n",
    "# 전체 인덱스\n",
    "all_indices = list(range(num_docs))\n",
    "\n",
    "# train/test 분할 (ex: 80/20)\n",
    "train_idx, test_idx = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# 저장 경로\n",
    "os.makedirs(\"data/jail_breaker\", exist_ok=True)\n",
    "\n",
    "# 저장\n",
    "with open(\"data/jail_breaker/train_idx.json\", \"w\") as f:\n",
    "    json.dump(train_idx, f)\n",
    "\n",
    "with open(\"data/jail_breaker/test_idx.json\", \"w\") as f:\n",
    "    json.dump(test_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f28beb",
   "metadata": {},
   "source": [
    "##### Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e662d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Shape Check\n",
    "for x in glob.glob('data/jail_breaker_data/*'):\n",
    "    with open(x, 'rb') as fp:\n",
    "        if 'pkl' in x:\n",
    "            adj = pickle.load(fp)\n",
    "            print(x)\n",
    "            print(type(adj))         # <class 'scipy.sparse.coo.coo_matrix'>\n",
    "            print(adj.shape)         # (예: (1000, 500))\n",
    "            # print(adj.nnz)           # non-zero 원소 수\n",
    "            \n",
    "        elif 'json' in x:\n",
    "            print(x)\n",
    "            x = json.load(fp)\n",
    "            print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7e48395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "jail_breaker\n",
      "True\n",
      "10000\n",
      "231\n",
      "211\n",
      "9558\n",
      "data process time: 2.0872080326080322\n",
      "Epoch 1, train_loss: 13.0739, train_acc: 0.0866, time: 0.5890\n",
      "\n",
      "Epoch 2, train_loss: 12.7130, train_acc: 0.1861, time: 0.2243\n",
      "\n",
      "Epoch 3, train_loss: 12.5218, train_acc: 0.1558, time: 0.2233\n",
      "\n",
      "Epoch 4, train_loss: 12.3863, train_acc: 0.2424, time: 0.2233\n",
      "\n",
      "Epoch 5, train_loss: 12.2864, train_acc: 0.2771, time: 0.2237\n",
      "\n",
      "Valid  loss: 2.2574  acc: 0.3270  f1: 0.2725\n",
      "Test  loss: 2.3658 acc: 0.3001 f1: 0.1849 time: 0.0759\n",
      "Epoch 6, train_loss: 12.2712, train_acc: 0.2944, time: 0.2233\n",
      "\n",
      "Epoch 7, train_loss: 12.2765, train_acc: 0.3117, time: 0.2231\n",
      "\n",
      "Epoch 8, train_loss: 12.2251, train_acc: 0.3593, time: 0.2232\n",
      "\n",
      "Epoch 9, train_loss: 12.2202, train_acc: 0.3723, time: 0.2234\n",
      "\n",
      "Epoch 10, train_loss: 12.2220, train_acc: 0.3420, time: 0.2233\n",
      "\n",
      "Valid  loss: 2.2221  acc: 0.3412  f1: 0.2910\n",
      "Test  loss: 2.2848 acc: 0.3661 f1: 0.2208 time: 0.0756\n",
      "Epoch 11, train_loss: 12.1807, train_acc: 0.3420, time: 0.2232\n",
      "\n",
      "Epoch 12, train_loss: 12.2144, train_acc: 0.3680, time: 0.2233\n",
      "\n",
      "Epoch 13, train_loss: 12.1606, train_acc: 0.3983, time: 0.2234\n",
      "\n",
      "Epoch 14, train_loss: 12.1729, train_acc: 0.3983, time: 0.2231\n",
      "\n",
      "Epoch 15, train_loss: 12.1275, train_acc: 0.3853, time: 0.2231\n",
      "\n",
      "Valid  loss: 2.2067  acc: 0.4028  f1: 0.3602\n",
      "Test  loss: 2.2466 acc: 0.4207 f1: 0.2544 time: 0.0757\n",
      "Epoch 16, train_loss: 12.1212, train_acc: 0.3506, time: 0.2234\n",
      "\n",
      "Epoch 17, train_loss: 12.0930, train_acc: 0.4242, time: 0.2233\n",
      "\n",
      "Epoch 18, train_loss: 12.1124, train_acc: 0.4329, time: 0.2234\n",
      "\n",
      "Epoch 19, train_loss: 12.1228, train_acc: 0.3463, time: 0.2234\n",
      "\n",
      "Epoch 20, train_loss: 12.0583, train_acc: 0.4113, time: 0.2235\n",
      "\n",
      "Valid  loss: 2.1935  acc: 0.4076  f1: 0.3589\n",
      "Test  loss: 2.2290 acc: 0.4571 f1: 0.2697 time: 0.0753\n",
      "Epoch 21, train_loss: 12.0768, train_acc: 0.4762, time: 0.2243\n",
      "\n",
      "Epoch 22, train_loss: 12.0471, train_acc: 0.4632, time: 0.2240\n",
      "\n",
      "Epoch 23, train_loss: 12.0732, train_acc: 0.4805, time: 0.2240\n",
      "\n",
      "Epoch 24, train_loss: 12.0333, train_acc: 0.4892, time: 0.2241\n",
      "\n",
      "Epoch 25, train_loss: 12.0388, train_acc: 0.4978, time: 0.2240\n",
      "\n",
      "Valid  loss: 2.1840  acc: 0.4028  f1: 0.3703\n",
      "Test  loss: 2.2145 acc: 0.4709 f1: 0.2641 time: 0.0759\n",
      "Epoch 26, train_loss: 12.0433, train_acc: 0.5108, time: 0.2245\n",
      "\n",
      "Epoch 27, train_loss: 12.0392, train_acc: 0.4892, time: 0.2241\n",
      "\n",
      "Epoch 28, train_loss: 12.0363, train_acc: 0.5584, time: 0.2238\n",
      "\n",
      "Epoch 29, train_loss: 11.9712, train_acc: 0.5195, time: 0.2240\n",
      "\n",
      "Epoch 30, train_loss: 11.9957, train_acc: 0.5758, time: 0.2239\n",
      "\n",
      "Valid  loss: 2.1739  acc: 0.4265  f1: 0.3995\n",
      "Test  loss: 2.2073 acc: 0.4774 f1: 0.2775 time: 0.0760\n",
      "Epoch 31, train_loss: 11.9875, train_acc: 0.5541, time: 0.2239\n",
      "\n",
      "Epoch 32, train_loss: 11.9661, train_acc: 0.6017, time: 0.2241\n",
      "\n",
      "Epoch 33, train_loss: 11.9200, train_acc: 0.6320, time: 0.2240\n",
      "\n",
      "Epoch 34, train_loss: 11.8962, train_acc: 0.6017, time: 0.2238\n",
      "\n",
      "Epoch 35, train_loss: 11.9203, train_acc: 0.6017, time: 0.2239\n",
      "\n",
      "Valid  loss: 2.1702  acc: 0.4645  f1: 0.4523\n",
      "Test  loss: 2.1948 acc: 0.4844 f1: 0.3277 time: 0.0761\n",
      "Epoch 36, train_loss: 11.9328, train_acc: 0.6104, time: 0.2242\n",
      "\n",
      "Epoch 37, train_loss: 11.8901, train_acc: 0.6190, time: 0.2244\n",
      "\n",
      "Epoch 38, train_loss: 11.8623, train_acc: 0.6407, time: 0.2242\n",
      "\n",
      "Epoch 39, train_loss: 11.8699, train_acc: 0.6234, time: 0.2240\n",
      "\n",
      "Epoch 40, train_loss: 11.8663, train_acc: 0.6450, time: 0.2240\n",
      "\n",
      "Valid  loss: 2.1657  acc: 0.4550  f1: 0.4359\n",
      "Test  loss: 2.1738 acc: 0.4867 f1: 0.3146 time: 0.0761\n",
      "Epoch 41, train_loss: 11.8747, train_acc: 0.6494, time: 0.2244\n",
      "\n",
      "Epoch 42, train_loss: 11.8491, train_acc: 0.6407, time: 0.2242\n",
      "\n",
      "Epoch 43, train_loss: 11.9184, train_acc: 0.6667, time: 0.2244\n",
      "\n",
      "Epoch 44, train_loss: 11.8653, train_acc: 0.6104, time: 0.2242\n",
      "\n",
      "Epoch 45, train_loss: 11.8697, train_acc: 0.7229, time: 0.2239\n",
      "\n",
      "Valid  loss: 2.1511  acc: 0.4455  f1: 0.4237\n",
      "Test  loss: 2.1517 acc: 0.5010 f1: 0.3272 time: 0.0758\n",
      "Epoch 46, train_loss: 11.8116, train_acc: 0.6970, time: 0.2242\n",
      "\n",
      "Epoch 47, train_loss: 11.7685, train_acc: 0.6926, time: 0.2239\n",
      "\n",
      "Epoch 48, train_loss: 11.7857, train_acc: 0.7100, time: 0.2243\n",
      "\n",
      "Epoch 49, train_loss: 11.7181, train_acc: 0.7229, time: 0.2246\n",
      "\n",
      "Epoch 50, train_loss: 11.7323, train_acc: 0.7229, time: 0.2242\n",
      "\n",
      "Valid  loss: 2.1558  acc: 0.4645  f1: 0.4545\n",
      "Test  loss: 2.1372 acc: 0.4990 f1: 0.3076 time: 0.0759\n",
      "Epoch 51, train_loss: 11.7160, train_acc: 0.7359, time: 0.2248\n",
      "\n",
      "Epoch 52, train_loss: 11.6711, train_acc: 0.7576, time: 0.2245\n",
      "\n",
      "Epoch 53, train_loss: 11.6663, train_acc: 0.7532, time: 0.2246\n",
      "\n",
      "Epoch 54, train_loss: 11.6953, train_acc: 0.7143, time: 0.2250\n",
      "\n",
      "Epoch 55, train_loss: 11.6703, train_acc: 0.7965, time: 0.2246\n",
      "\n",
      "Valid  loss: 2.1545  acc: 0.4455  f1: 0.4303\n",
      "Test  loss: 2.1347 acc: 0.5096 f1: 0.3124 time: 0.0760\n",
      "Epoch 56, train_loss: 11.6546, train_acc: 0.7532, time: 0.2254\n",
      "\n",
      "Epoch 57, train_loss: 11.6168, train_acc: 0.7489, time: 0.2255\n",
      "\n",
      "Epoch 58, train_loss: 11.6303, train_acc: 0.7662, time: 0.2251\n",
      "\n",
      "Epoch 59, train_loss: 11.6247, train_acc: 0.7922, time: 0.2251\n",
      "\n",
      "Epoch 60, train_loss: 11.5941, train_acc: 0.7792, time: 0.2248\n",
      "\n",
      "Valid  loss: 2.1714  acc: 0.4550  f1: 0.4520\n",
      "Test  loss: 2.1389 acc: 0.5115 f1: 0.2854 time: 0.0771\n",
      "Epoch 61, train_loss: 11.5637, train_acc: 0.7835, time: 0.2248\n",
      "\n",
      "Epoch 62, train_loss: 11.5703, train_acc: 0.8009, time: 0.2243\n",
      "\n",
      "Epoch 63, train_loss: 11.6264, train_acc: 0.7965, time: 0.2248\n",
      "\n",
      "Epoch 64, train_loss: 11.6431, train_acc: 0.7922, time: 0.2242\n",
      "\n",
      "Epoch 65, train_loss: 11.5401, train_acc: 0.8182, time: 0.2240\n",
      "\n",
      "Valid  loss: 2.1647  acc: 0.4123  f1: 0.4017\n",
      "Test  loss: 2.1187 acc: 0.5166 f1: 0.2926 time: 0.0761\n",
      "Epoch 66, train_loss: 11.6567, train_acc: 0.8052, time: 0.2245\n",
      "\n",
      "Epoch 67, train_loss: 11.6140, train_acc: 0.7965, time: 0.2244\n",
      "\n",
      "Epoch 68, train_loss: 11.6065, train_acc: 0.8139, time: 0.2247\n",
      "\n",
      "Epoch 69, train_loss: 11.5598, train_acc: 0.8225, time: 0.2253\n",
      "\n",
      "Epoch 70, train_loss: 11.5600, train_acc: 0.8312, time: 0.2245\n",
      "\n",
      "Valid  loss: 2.1776  acc: 0.4550  f1: 0.4540\n",
      "Test  loss: 2.1375 acc: 0.5031 f1: 0.2876 time: 0.0762\n",
      "Epoch 71, train_loss: 11.5621, train_acc: 0.8312, time: 0.2256\n",
      "\n",
      "Epoch 72, train_loss: 11.5871, train_acc: 0.8225, time: 0.2252\n",
      "\n",
      "Epoch 73, train_loss: 11.5354, train_acc: 0.8139, time: 0.2255\n",
      "\n",
      "Epoch 74, train_loss: 11.5805, train_acc: 0.8225, time: 0.2247\n",
      "\n",
      "Epoch 75, train_loss: 11.6404, train_acc: 0.8139, time: 0.2245\n",
      "\n",
      "Valid  loss: 2.1642  acc: 0.4645  f1: 0.4589\n",
      "Test  loss: 2.1343 acc: 0.4902 f1: 0.2830 time: 0.0762\n",
      "Epoch 76, train_loss: 11.5554, train_acc: 0.8745, time: 0.2247\n",
      "\n",
      "Epoch 77, train_loss: 11.5306, train_acc: 0.8571, time: 0.2246\n",
      "\n",
      "Epoch 78, train_loss: 11.5904, train_acc: 0.8182, time: 0.2247\n",
      "\n",
      "Epoch 79, train_loss: 11.5448, train_acc: 0.8658, time: 0.2243\n",
      "\n",
      "Epoch 80, train_loss: 11.5196, train_acc: 0.8485, time: 0.2247\n",
      "\n",
      "Valid  loss: 2.1759  acc: 0.4502  f1: 0.4451\n",
      "Test  loss: 2.1464 acc: 0.4963 f1: 0.2778 time: 0.0761\n",
      "Epoch 81, train_loss: 11.6350, train_acc: 0.8658, time: 0.2247\n",
      "\n",
      "Epoch 82, train_loss: 11.4706, train_acc: 0.8485, time: 0.2247\n",
      "\n",
      "Epoch 83, train_loss: 11.4577, train_acc: 0.8701, time: 0.2247\n",
      "\n",
      "Epoch 84, train_loss: 11.4958, train_acc: 0.8658, time: 0.2245\n",
      "\n",
      "Epoch 85, train_loss: 11.5389, train_acc: 0.8701, time: 0.2248\n",
      "\n",
      "Valid  loss: 2.1795  acc: 0.4455  f1: 0.4437\n",
      "Test  loss: 2.1462 acc: 0.5022 f1: 0.2883 time: 0.0762\n",
      "Epoch 86, train_loss: 11.6160, train_acc: 0.8398, time: 0.2246\n",
      "\n",
      "Epoch 87, train_loss: 11.4778, train_acc: 0.8485, time: 0.2247\n",
      "\n",
      "Epoch 88, train_loss: 11.4494, train_acc: 0.8528, time: 0.2249\n",
      "\n",
      "Epoch 89, train_loss: 11.5027, train_acc: 0.8961, time: 0.2245\n",
      "\n",
      "Epoch 90, train_loss: 11.4470, train_acc: 0.8961, time: 0.2244\n",
      "\n",
      "Valid  loss: 2.1729  acc: 0.4787  f1: 0.4802\n",
      "Test  loss: 2.1427 acc: 0.4913 f1: 0.2872 time: 0.0761\n",
      "Epoch 91, train_loss: 11.4520, train_acc: 0.8312, time: 0.2246\n",
      "\n",
      "Epoch 92, train_loss: 11.4319, train_acc: 0.8961, time: 0.2245\n",
      "\n",
      "Epoch 93, train_loss: 11.6624, train_acc: 0.8442, time: 0.2252\n",
      "\n",
      "Epoch 94, train_loss: 11.5868, train_acc: 0.8961, time: 0.2249\n",
      "\n",
      "Epoch 95, train_loss: 11.5175, train_acc: 0.8918, time: 0.2250\n",
      "\n",
      "Valid  loss: 2.1810  acc: 0.4597  f1: 0.4295\n",
      "Test  loss: 2.1417 acc: 0.4944 f1: 0.2913 time: 0.0762\n",
      "Epoch 96, train_loss: 11.4887, train_acc: 0.8788, time: 0.2249\n",
      "\n",
      "Epoch 97, train_loss: 11.5187, train_acc: 0.9048, time: 0.2254\n",
      "\n",
      "Epoch 98, train_loss: 11.4303, train_acc: 0.8701, time: 0.2251\n",
      "\n",
      "Epoch 99, train_loss: 11.5236, train_acc: 0.8528, time: 0.2247\n",
      "\n",
      "Epoch 100, train_loss: 11.3823, train_acc: 0.8745, time: 0.2247\n",
      "\n",
      "Valid  loss: 2.1645  acc: 0.4645  f1: 0.4685\n",
      "Test  loss: 2.1316 acc: 0.5043 f1: 0.2939 time: 0.0760\n",
      "VALID: VALID ACC 0.4786730110645294  VALID F1 0.4802118689480108 EPOCH 90\n",
      "VALID: TEST ACC 0.49131616950035095 TEST F1 0.28719287563714785 EPOCH 90\n",
      "GLOBAL: TEST ACC 0.5166352987289429 TEST F1 0.2925633949128449 EPOCH 65\n",
      "Epoch 101, train_loss: 11.4692, train_acc: 0.8485, time: 0.2247\n",
      "\n",
      "Epoch 102, train_loss: 11.4285, train_acc: 0.8961, time: 0.2252\n",
      "\n",
      "Epoch 103, train_loss: 11.5281, train_acc: 0.8918, time: 0.2245\n",
      "\n",
      "Epoch 104, train_loss: 11.4769, train_acc: 0.8961, time: 0.2248\n",
      "\n",
      "Epoch 105, train_loss: 11.5415, train_acc: 0.8745, time: 0.2248\n",
      "\n",
      "Valid  loss: 2.1820  acc: 0.4692  f1: 0.4321\n",
      "Test  loss: 2.1296 acc: 0.4959 f1: 0.2828 time: 0.0761\n",
      "Epoch 106, train_loss: 11.4465, train_acc: 0.8874, time: 0.2250\n",
      "\n",
      "Epoch 107, train_loss: 11.3733, train_acc: 0.8701, time: 0.2248\n",
      "\n",
      "Epoch 108, train_loss: 11.5013, train_acc: 0.8485, time: 0.2244\n",
      "\n",
      "Epoch 109, train_loss: 11.3735, train_acc: 0.9091, time: 0.2246\n",
      "\n",
      "Epoch 110, train_loss: 11.3947, train_acc: 0.8831, time: 0.2250\n",
      "\n",
      "Valid  loss: 2.1869  acc: 0.4787  f1: 0.4802\n",
      "Test  loss: 2.1358 acc: 0.4891 f1: 0.2784 time: 0.0760\n",
      "Epoch 111, train_loss: 11.4177, train_acc: 0.9048, time: 0.2249\n",
      "\n",
      "Epoch 112, train_loss: 11.3777, train_acc: 0.9177, time: 0.2250\n",
      "\n",
      "Epoch 113, train_loss: 11.4195, train_acc: 0.8745, time: 0.2245\n",
      "\n",
      "Epoch 114, train_loss: 11.3930, train_acc: 0.8831, time: 0.2248\n",
      "\n",
      "Epoch 115, train_loss: 11.4614, train_acc: 0.8701, time: 0.2251\n",
      "\n",
      "Valid  loss: 2.1855  acc: 0.4645  f1: 0.4318\n",
      "Test  loss: 2.1324 acc: 0.4945 f1: 0.2895 time: 0.0761\n",
      "Epoch 116, train_loss: 11.4242, train_acc: 0.9004, time: 0.2249\n",
      "\n",
      "Epoch 117, train_loss: 11.4108, train_acc: 0.8831, time: 0.2246\n",
      "\n",
      "Epoch 118, train_loss: 11.4223, train_acc: 0.9091, time: 0.2246\n",
      "\n",
      "Epoch 119, train_loss: 11.4115, train_acc: 0.8918, time: 0.2253\n",
      "\n",
      "Epoch 120, train_loss: 11.4360, train_acc: 0.9134, time: 0.2258\n",
      "\n",
      "Valid  loss: 2.1743  acc: 0.4739  f1: 0.4411\n",
      "Test  loss: 2.1354 acc: 0.5061 f1: 0.2986 time: 0.0766\n",
      "Epoch 121, train_loss: 11.3519, train_acc: 0.9091, time: 0.2259\n",
      "\n",
      "Epoch 122, train_loss: 11.4044, train_acc: 0.8745, time: 0.2250\n",
      "\n",
      "Epoch 123, train_loss: 11.4521, train_acc: 0.9134, time: 0.2253\n",
      "\n",
      "Epoch 124, train_loss: 11.4539, train_acc: 0.9134, time: 0.2259\n",
      "\n",
      "Epoch 125, train_loss: 11.4271, train_acc: 0.9134, time: 0.2249\n",
      "\n",
      "Valid  loss: 2.1908  acc: 0.4739  f1: 0.4376\n",
      "Test  loss: 2.1425 acc: 0.4841 f1: 0.2862 time: 0.0760\n",
      "Epoch 126, train_loss: 11.3932, train_acc: 0.8874, time: 0.2252\n",
      "\n",
      "Epoch 127, train_loss: 11.4204, train_acc: 0.8701, time: 0.2249\n",
      "\n",
      "Epoch 128, train_loss: 11.3336, train_acc: 0.9004, time: 0.2251\n",
      "\n",
      "Epoch 129, train_loss: 11.4377, train_acc: 0.9004, time: 0.2254\n",
      "\n",
      "Epoch 130, train_loss: 11.3575, train_acc: 0.9004, time: 0.2250\n",
      "\n",
      "Valid  loss: 2.1928  acc: 0.4645  f1: 0.4666\n",
      "Test  loss: 2.1422 acc: 0.4950 f1: 0.2870 time: 0.0761\n",
      "Epoch 131, train_loss: 11.3412, train_acc: 0.8918, time: 0.2258\n",
      "\n",
      "Epoch 132, train_loss: 11.3246, train_acc: 0.8918, time: 0.2257\n",
      "\n",
      "Epoch 133, train_loss: 11.4621, train_acc: 0.8874, time: 0.2263\n",
      "\n",
      "Epoch 134, train_loss: 11.3129, train_acc: 0.8918, time: 0.2252\n",
      "\n",
      "Epoch 135, train_loss: 11.4302, train_acc: 0.8961, time: 0.2251\n",
      "\n",
      "Valid  loss: 2.1744  acc: 0.4787  f1: 0.4824\n",
      "Test  loss: 2.1417 acc: 0.4887 f1: 0.2858 time: 0.0764\n",
      "Epoch 136, train_loss: 11.3686, train_acc: 0.8788, time: 0.2253\n",
      "\n",
      "Epoch 137, train_loss: 11.3285, train_acc: 0.9177, time: 0.2254\n",
      "\n",
      "Epoch 138, train_loss: 11.6093, train_acc: 0.8918, time: 0.2250\n",
      "\n",
      "Epoch 139, train_loss: 11.4372, train_acc: 0.9091, time: 0.2253\n",
      "\n",
      "Epoch 140, train_loss: 11.3536, train_acc: 0.9091, time: 0.2253\n",
      "\n",
      "Valid  loss: 2.1767  acc: 0.4645  f1: 0.4328\n",
      "Test  loss: 2.1219 acc: 0.4938 f1: 0.2942 time: 0.0762\n",
      "Epoch 141, train_loss: 11.3321, train_acc: 0.9004, time: 0.2254\n",
      "\n",
      "Epoch 142, train_loss: 11.3797, train_acc: 0.9048, time: 0.2255\n",
      "\n",
      "Epoch 143, train_loss: 11.3407, train_acc: 0.8961, time: 0.2250\n",
      "\n",
      "Epoch 144, train_loss: 11.3359, train_acc: 0.9091, time: 0.2253\n",
      "\n",
      "Epoch 145, train_loss: 11.3293, train_acc: 0.9134, time: 0.2253\n",
      "\n",
      "Valid  loss: 2.1826  acc: 0.4645  f1: 0.4283\n",
      "Test  loss: 2.1293 acc: 0.4939 f1: 0.2888 time: 0.0761\n",
      "Epoch 146, train_loss: 11.3116, train_acc: 0.9048, time: 0.2255\n",
      "\n",
      "Epoch 147, train_loss: 11.3398, train_acc: 0.9091, time: 0.2251\n",
      "\n",
      "Epoch 148, train_loss: 11.3425, train_acc: 0.9048, time: 0.2254\n",
      "\n",
      "Epoch 149, train_loss: 11.4046, train_acc: 0.9004, time: 0.2256\n",
      "\n",
      "Epoch 150, train_loss: 11.2931, train_acc: 0.8874, time: 0.2256\n",
      "\n",
      "Valid  loss: 2.1775  acc: 0.4929  f1: 0.4969\n",
      "Test  loss: 2.1304 acc: 0.5088 f1: 0.2957 time: 0.0760\n",
      "Epoch 151, train_loss: 11.3511, train_acc: 0.8961, time: 0.2253\n",
      "\n",
      "Epoch 152, train_loss: 11.2867, train_acc: 0.8961, time: 0.2252\n",
      "\n",
      "Epoch 153, train_loss: 11.3619, train_acc: 0.8918, time: 0.2254\n",
      "\n",
      "Epoch 154, train_loss: 11.3566, train_acc: 0.9048, time: 0.2259\n",
      "\n",
      "Epoch 155, train_loss: 11.3274, train_acc: 0.9177, time: 0.2251\n",
      "\n",
      "Valid  loss: 2.1787  acc: 0.4692  f1: 0.4361\n",
      "Test  loss: 2.1239 acc: 0.5113 f1: 0.3066 time: 0.0766\n",
      "Epoch 156, train_loss: 11.3456, train_acc: 0.9134, time: 0.2256\n",
      "\n",
      "Epoch 157, train_loss: 11.3128, train_acc: 0.9221, time: 0.2254\n",
      "\n",
      "Epoch 158, train_loss: 11.3246, train_acc: 0.8918, time: 0.2261\n",
      "\n",
      "Epoch 159, train_loss: 11.3205, train_acc: 0.9134, time: 0.2252\n",
      "\n",
      "Epoch 160, train_loss: 11.2854, train_acc: 0.9091, time: 0.2254\n",
      "\n",
      "Valid  loss: 2.1848  acc: 0.4645  f1: 0.4280\n",
      "Test  loss: 2.1199 acc: 0.4967 f1: 0.2903 time: 0.0765\n",
      "Epoch 161, train_loss: 11.2873, train_acc: 0.9177, time: 0.2253\n",
      "\n",
      "Epoch 162, train_loss: 11.2998, train_acc: 0.9004, time: 0.2254\n",
      "\n",
      "Epoch 163, train_loss: 11.2958, train_acc: 0.9004, time: 0.2253\n",
      "\n",
      "Epoch 164, train_loss: 11.3325, train_acc: 0.9221, time: 0.2253\n",
      "\n",
      "Epoch 165, train_loss: 11.3285, train_acc: 0.9048, time: 0.2255\n",
      "\n",
      "Valid  loss: 2.1843  acc: 0.4834  f1: 0.4442\n",
      "Test  loss: 2.1111 acc: 0.5042 f1: 0.2885 time: 0.0763\n",
      "Epoch 166, train_loss: 11.3263, train_acc: 0.9134, time: 0.2256\n",
      "\n",
      "Epoch 167, train_loss: 11.3557, train_acc: 0.9177, time: 0.2255\n",
      "\n",
      "Epoch 168, train_loss: 11.3643, train_acc: 0.9307, time: 0.2253\n",
      "\n",
      "Epoch 169, train_loss: 11.3669, train_acc: 0.9134, time: 0.2253\n",
      "\n",
      "Epoch 170, train_loss: 11.2626, train_acc: 0.9091, time: 0.2257\n",
      "\n",
      "Valid  loss: 2.1829  acc: 0.4787  f1: 0.4778\n",
      "Test  loss: 2.1183 acc: 0.5084 f1: 0.2881 time: 0.0760\n",
      "Epoch 171, train_loss: 11.3106, train_acc: 0.9264, time: 0.2256\n",
      "\n",
      "Epoch 172, train_loss: 11.3020, train_acc: 0.9134, time: 0.2254\n",
      "\n",
      "Epoch 173, train_loss: 11.3212, train_acc: 0.9177, time: 0.2255\n",
      "\n",
      "Epoch 174, train_loss: 11.2825, train_acc: 0.9004, time: 0.2259\n",
      "\n",
      "Epoch 175, train_loss: 11.2492, train_acc: 0.9221, time: 0.2257\n",
      "\n",
      "Valid  loss: 2.1859  acc: 0.4739  f1: 0.4398\n",
      "Test  loss: 2.1217 acc: 0.4909 f1: 0.2862 time: 0.0766\n",
      "Epoch 176, train_loss: 11.2897, train_acc: 0.9177, time: 0.2257\n",
      "\n",
      "Epoch 177, train_loss: 11.2771, train_acc: 0.9134, time: 0.2255\n",
      "\n",
      "Epoch 178, train_loss: 11.3001, train_acc: 0.9177, time: 0.2256\n",
      "\n",
      "Epoch 179, train_loss: 11.2898, train_acc: 0.8701, time: 0.2253\n",
      "\n",
      "Epoch 180, train_loss: 11.3235, train_acc: 0.8831, time: 0.2253\n",
      "\n",
      "Valid  loss: 2.1800  acc: 0.4502  f1: 0.4510\n",
      "Test  loss: 2.1216 acc: 0.5069 f1: 0.2919 time: 0.0770\n",
      "Epoch 181, train_loss: 11.3949, train_acc: 0.9264, time: 0.2259\n",
      "\n",
      "Epoch 182, train_loss: 11.3751, train_acc: 0.9351, time: 0.2260\n",
      "\n",
      "Epoch 183, train_loss: 11.2809, train_acc: 0.9048, time: 0.2263\n",
      "\n",
      "Epoch 184, train_loss: 11.3222, train_acc: 0.9177, time: 0.2256\n",
      "\n",
      "Epoch 185, train_loss: 11.3084, train_acc: 0.9004, time: 0.2258\n",
      "\n",
      "Valid  loss: 2.1782  acc: 0.4645  f1: 0.4667\n",
      "Test  loss: 2.1151 acc: 0.5159 f1: 0.2928 time: 0.0763\n",
      "Epoch 186, train_loss: 11.2209, train_acc: 0.9048, time: 0.2259\n",
      "\n",
      "Epoch 187, train_loss: 11.3455, train_acc: 0.8831, time: 0.2257\n",
      "\n",
      "Epoch 188, train_loss: 11.2698, train_acc: 0.9177, time: 0.2257\n",
      "\n",
      "Epoch 189, train_loss: 11.2699, train_acc: 0.9048, time: 0.2262\n",
      "\n",
      "Epoch 190, train_loss: 11.2583, train_acc: 0.9091, time: 0.2255\n",
      "\n",
      "Valid  loss: 2.1780  acc: 0.4550  f1: 0.4067\n",
      "Test  loss: 2.1269 acc: 0.4947 f1: 0.2769 time: 0.0763\n",
      "Epoch 191, train_loss: 11.3697, train_acc: 0.9264, time: 0.2256\n",
      "\n",
      "Epoch 192, train_loss: 11.2226, train_acc: 0.9264, time: 0.2257\n",
      "\n",
      "Epoch 193, train_loss: 11.2494, train_acc: 0.9264, time: 0.2261\n",
      "\n",
      "Epoch 194, train_loss: 11.2480, train_acc: 0.9351, time: 0.2255\n",
      "\n",
      "Epoch 195, train_loss: 11.2984, train_acc: 0.9048, time: 0.2255\n",
      "\n",
      "Valid  loss: 2.1712  acc: 0.4692  f1: 0.4315\n",
      "Test  loss: 2.1152 acc: 0.5139 f1: 0.2970 time: 0.0767\n",
      "Epoch 196, train_loss: 11.3696, train_acc: 0.9048, time: 0.2257\n",
      "\n",
      "Epoch 197, train_loss: 11.2619, train_acc: 0.9048, time: 0.2263\n",
      "\n",
      "Epoch 198, train_loss: 11.3027, train_acc: 0.9134, time: 0.2258\n",
      "\n",
      "Epoch 199, train_loss: 11.3465, train_acc: 0.9004, time: 0.2258\n",
      "\n",
      "Epoch 200, train_loss: 11.3169, train_acc: 0.9134, time: 0.2261\n",
      "\n",
      "Valid  loss: 2.1702  acc: 0.4597  f1: 0.4632\n",
      "Test  loss: 2.1131 acc: 0.5119 f1: 0.2945 time: 0.0764\n",
      "VALID: VALID ACC 0.49289101362228394  VALID F1 0.4969021133642839 EPOCH 150\n",
      "VALID: TEST ACC 0.5087884664535522 TEST F1 0.2956570477202706 EPOCH 150\n",
      "GLOBAL: TEST ACC 0.5166352987289429 TEST F1 0.2925633949128449 EPOCH 65\n",
      "Epoch 201, train_loss: 11.2775, train_acc: 0.9091, time: 0.2258\n",
      "\n",
      "Epoch 202, train_loss: 11.2858, train_acc: 0.9091, time: 0.2256\n",
      "\n",
      "Epoch 203, train_loss: 11.2934, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 204, train_loss: 11.2860, train_acc: 0.9091, time: 0.2259\n",
      "\n",
      "Epoch 205, train_loss: 11.3157, train_acc: 0.9048, time: 0.2260\n",
      "\n",
      "Valid  loss: 2.1757  acc: 0.4739  f1: 0.4753\n",
      "Test  loss: 2.1214 acc: 0.5121 f1: 0.2930 time: 0.0764\n",
      "Epoch 206, train_loss: 11.4526, train_acc: 0.9264, time: 0.2257\n",
      "\n",
      "Epoch 207, train_loss: 11.2845, train_acc: 0.9264, time: 0.2256\n",
      "\n",
      "Epoch 208, train_loss: 11.2696, train_acc: 0.9048, time: 0.2260\n",
      "\n",
      "Epoch 209, train_loss: 11.2534, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 210, train_loss: 11.2474, train_acc: 0.9004, time: 0.2261\n",
      "\n",
      "Valid  loss: 2.1800  acc: 0.4834  f1: 0.4805\n",
      "Test  loss: 2.1326 acc: 0.4976 f1: 0.2792 time: 0.0768\n",
      "Epoch 211, train_loss: 11.2810, train_acc: 0.9091, time: 0.2257\n",
      "\n",
      "Epoch 212, train_loss: 11.2914, train_acc: 0.9134, time: 0.2261\n",
      "\n",
      "Epoch 213, train_loss: 11.2618, train_acc: 0.9221, time: 0.2256\n",
      "\n",
      "Epoch 214, train_loss: 11.4482, train_acc: 0.9177, time: 0.2257\n",
      "\n",
      "Epoch 215, train_loss: 11.2614, train_acc: 0.9177, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1891  acc: 0.4882  f1: 0.4398\n",
      "Test  loss: 2.1306 acc: 0.4931 f1: 0.2747 time: 0.0765\n",
      "Epoch 216, train_loss: 11.2901, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 217, train_loss: 11.2479, train_acc: 0.9264, time: 0.2254\n",
      "\n",
      "Epoch 218, train_loss: 11.2592, train_acc: 0.9177, time: 0.2258\n",
      "\n",
      "Epoch 219, train_loss: 11.3171, train_acc: 0.8874, time: 0.2261\n",
      "\n",
      "Epoch 220, train_loss: 11.2853, train_acc: 0.9221, time: 0.2256\n",
      "\n",
      "Valid  loss: 2.1755  acc: 0.4929  f1: 0.4981\n",
      "Test  loss: 2.1104 acc: 0.5107 f1: 0.2968 time: 0.0765\n",
      "Epoch 221, train_loss: 11.2624, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 222, train_loss: 11.2665, train_acc: 0.9177, time: 0.2258\n",
      "\n",
      "Epoch 223, train_loss: 11.2418, train_acc: 0.9177, time: 0.2262\n",
      "\n",
      "Epoch 224, train_loss: 11.2981, train_acc: 0.9177, time: 0.2261\n",
      "\n",
      "Epoch 225, train_loss: 11.2896, train_acc: 0.9177, time: 0.2260\n",
      "\n",
      "Valid  loss: 2.1721  acc: 0.4692  f1: 0.4726\n",
      "Test  loss: 2.1136 acc: 0.5090 f1: 0.2872 time: 0.0768\n",
      "Epoch 226, train_loss: 11.2816, train_acc: 0.9264, time: 0.2257\n",
      "\n",
      "Epoch 227, train_loss: 11.3106, train_acc: 0.9134, time: 0.2263\n",
      "\n",
      "Epoch 228, train_loss: 11.2659, train_acc: 0.9264, time: 0.2256\n",
      "\n",
      "Epoch 229, train_loss: 11.2999, train_acc: 0.9264, time: 0.2258\n",
      "\n",
      "Epoch 230, train_loss: 11.2336, train_acc: 0.9264, time: 0.2260\n",
      "\n",
      "Valid  loss: 2.1742  acc: 0.4739  f1: 0.4771\n",
      "Test  loss: 2.1159 acc: 0.5077 f1: 0.2903 time: 0.0762\n",
      "Epoch 231, train_loss: 11.2581, train_acc: 0.9307, time: 0.2256\n",
      "\n",
      "Epoch 232, train_loss: 11.2243, train_acc: 0.9177, time: 0.2267\n",
      "\n",
      "Epoch 233, train_loss: 11.2666, train_acc: 0.9134, time: 0.2267\n",
      "\n",
      "Epoch 234, train_loss: 11.2298, train_acc: 0.9177, time: 0.2262\n",
      "\n",
      "Epoch 235, train_loss: 11.3014, train_acc: 0.9177, time: 0.2261\n",
      "\n",
      "Valid  loss: 2.1840  acc: 0.4645  f1: 0.4162\n",
      "Test  loss: 2.1122 acc: 0.5117 f1: 0.2805 time: 0.0764\n",
      "Epoch 236, train_loss: 11.2695, train_acc: 0.9177, time: 0.2267\n",
      "\n",
      "Epoch 237, train_loss: 11.3247, train_acc: 0.9221, time: 0.2271\n",
      "\n",
      "Epoch 238, train_loss: 11.2696, train_acc: 0.9307, time: 0.2264\n",
      "\n",
      "Epoch 239, train_loss: 11.2393, train_acc: 0.9134, time: 0.2265\n",
      "\n",
      "Epoch 240, train_loss: 11.2661, train_acc: 0.9134, time: 0.2266\n",
      "\n",
      "Valid  loss: 2.1826  acc: 0.4692  f1: 0.4733\n",
      "Test  loss: 2.1047 acc: 0.5153 f1: 0.2897 time: 0.0766\n",
      "Epoch 241, train_loss: 11.2575, train_acc: 0.9048, time: 0.2270\n",
      "\n",
      "Epoch 242, train_loss: 11.2841, train_acc: 0.9004, time: 0.2268\n",
      "\n",
      "Epoch 243, train_loss: 11.2474, train_acc: 0.9221, time: 0.2266\n",
      "\n",
      "Epoch 244, train_loss: 11.2575, train_acc: 0.9177, time: 0.2266\n",
      "\n",
      "Epoch 245, train_loss: 11.1930, train_acc: 0.9177, time: 0.2274\n",
      "\n",
      "Valid  loss: 2.1698  acc: 0.4597  f1: 0.4612\n",
      "Test  loss: 2.0992 acc: 0.5277 f1: 0.2997 time: 0.0768\n",
      "Epoch 246, train_loss: 11.2978, train_acc: 0.9134, time: 0.2265\n",
      "\n",
      "Epoch 247, train_loss: 11.2611, train_acc: 0.9091, time: 0.2270\n",
      "\n",
      "Epoch 248, train_loss: 11.2641, train_acc: 0.9221, time: 0.2269\n",
      "\n",
      "Epoch 249, train_loss: 11.2372, train_acc: 0.9221, time: 0.2269\n",
      "\n",
      "Epoch 250, train_loss: 11.2277, train_acc: 0.9091, time: 0.2274\n",
      "\n",
      "Valid  loss: 2.1704  acc: 0.4597  f1: 0.4242\n",
      "Test  loss: 2.1017 acc: 0.5319 f1: 0.2992 time: 0.0768\n",
      "Epoch 251, train_loss: 11.2511, train_acc: 0.9307, time: 0.2270\n",
      "\n",
      "Epoch 252, train_loss: 11.2133, train_acc: 0.9351, time: 0.2266\n",
      "\n",
      "Epoch 253, train_loss: 11.2339, train_acc: 0.9177, time: 0.2267\n",
      "\n",
      "Epoch 254, train_loss: 11.2302, train_acc: 0.9264, time: 0.2266\n",
      "\n",
      "Epoch 255, train_loss: 11.2657, train_acc: 0.9264, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1582  acc: 0.4787  f1: 0.4826\n",
      "Test  loss: 2.1060 acc: 0.5191 f1: 0.2894 time: 0.0769\n",
      "Epoch 256, train_loss: 11.2815, train_acc: 0.9264, time: 0.2263\n",
      "\n",
      "Epoch 257, train_loss: 11.1993, train_acc: 0.9091, time: 0.2262\n",
      "\n",
      "Epoch 258, train_loss: 11.2879, train_acc: 0.9264, time: 0.2266\n",
      "\n",
      "Epoch 259, train_loss: 11.2877, train_acc: 0.9177, time: 0.2269\n",
      "\n",
      "Epoch 260, train_loss: 11.3263, train_acc: 0.9177, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1720  acc: 0.4645  f1: 0.4298\n",
      "Test  loss: 2.1082 acc: 0.5273 f1: 0.2966 time: 0.0767\n",
      "Epoch 261, train_loss: 11.3400, train_acc: 0.9221, time: 0.2269\n",
      "\n",
      "Epoch 262, train_loss: 11.2596, train_acc: 0.9264, time: 0.2270\n",
      "\n",
      "Epoch 263, train_loss: 11.2394, train_acc: 0.9134, time: 0.2274\n",
      "\n",
      "Epoch 264, train_loss: 11.2070, train_acc: 0.9307, time: 0.2266\n",
      "\n",
      "Epoch 265, train_loss: 11.2031, train_acc: 0.9351, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1657  acc: 0.4929  f1: 0.4943\n",
      "Test  loss: 2.1100 acc: 0.5158 f1: 0.2845 time: 0.0768\n",
      "Epoch 266, train_loss: 11.2479, train_acc: 0.9351, time: 0.2266\n",
      "\n",
      "Epoch 267, train_loss: 11.2105, train_acc: 0.9307, time: 0.2266\n",
      "\n",
      "Epoch 268, train_loss: 11.2394, train_acc: 0.9177, time: 0.2266\n",
      "\n",
      "Epoch 269, train_loss: 11.2667, train_acc: 0.9177, time: 0.2272\n",
      "\n",
      "Epoch 270, train_loss: 11.2149, train_acc: 0.9307, time: 0.2263\n",
      "\n",
      "Valid  loss: 2.1725  acc: 0.4550  f1: 0.4196\n",
      "Test  loss: 2.1090 acc: 0.5161 f1: 0.2913 time: 0.0766\n",
      "Epoch 271, train_loss: 11.2456, train_acc: 0.9091, time: 0.2269\n",
      "\n",
      "Epoch 272, train_loss: 11.2162, train_acc: 0.9177, time: 0.2267\n",
      "\n",
      "Epoch 273, train_loss: 11.2458, train_acc: 0.9134, time: 0.2265\n",
      "\n",
      "Epoch 274, train_loss: 11.2479, train_acc: 0.9307, time: 0.2261\n",
      "\n",
      "Epoch 275, train_loss: 11.2065, train_acc: 0.9177, time: 0.2263\n",
      "\n",
      "Valid  loss: 2.1596  acc: 0.4787  f1: 0.4781\n",
      "Test  loss: 2.1027 acc: 0.5276 f1: 0.2892 time: 0.0763\n",
      "Epoch 276, train_loss: 11.2346, train_acc: 0.9048, time: 0.2267\n",
      "\n",
      "Epoch 277, train_loss: 11.2276, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Epoch 278, train_loss: 11.2210, train_acc: 0.9221, time: 0.2264\n",
      "\n",
      "Epoch 279, train_loss: 11.2029, train_acc: 0.9221, time: 0.2267\n",
      "\n",
      "Epoch 280, train_loss: 11.2756, train_acc: 0.9048, time: 0.2264\n",
      "\n",
      "Valid  loss: 2.1773  acc: 0.4645  f1: 0.4292\n",
      "Test  loss: 2.1030 acc: 0.5217 f1: 0.2943 time: 0.0769\n",
      "Epoch 281, train_loss: 11.2648, train_acc: 0.9091, time: 0.2263\n",
      "\n",
      "Epoch 282, train_loss: 11.2305, train_acc: 0.9264, time: 0.2266\n",
      "\n",
      "Epoch 283, train_loss: 11.2633, train_acc: 0.9221, time: 0.2262\n",
      "\n",
      "Epoch 284, train_loss: 11.2234, train_acc: 0.9177, time: 0.2267\n",
      "\n",
      "Epoch 285, train_loss: 11.2587, train_acc: 0.9134, time: 0.2270\n",
      "\n",
      "Valid  loss: 2.1705  acc: 0.4739  f1: 0.4763\n",
      "Test  loss: 2.1108 acc: 0.5245 f1: 0.2924 time: 0.0766\n",
      "Epoch 286, train_loss: 11.2835, train_acc: 0.9221, time: 0.2271\n",
      "\n",
      "Epoch 287, train_loss: 11.2206, train_acc: 0.8961, time: 0.2273\n",
      "\n",
      "Epoch 288, train_loss: 11.2180, train_acc: 0.9394, time: 0.2271\n",
      "\n",
      "Epoch 289, train_loss: 11.2626, train_acc: 0.9177, time: 0.2269\n",
      "\n",
      "Epoch 290, train_loss: 11.2228, train_acc: 0.9394, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1846  acc: 0.4455  f1: 0.4101\n",
      "Test  loss: 2.1177 acc: 0.5113 f1: 0.2825 time: 0.0770\n",
      "Epoch 291, train_loss: 11.2898, train_acc: 0.9307, time: 0.2266\n",
      "\n",
      "Epoch 292, train_loss: 11.2127, train_acc: 0.9221, time: 0.2265\n",
      "\n",
      "Epoch 293, train_loss: 11.2114, train_acc: 0.9307, time: 0.2269\n",
      "\n",
      "Epoch 294, train_loss: 11.2291, train_acc: 0.9307, time: 0.2269\n",
      "\n",
      "Epoch 295, train_loss: 11.1994, train_acc: 0.9177, time: 0.2273\n",
      "\n",
      "Valid  loss: 2.1675  acc: 0.4645  f1: 0.4682\n",
      "Test  loss: 2.0893 acc: 0.5234 f1: 0.2891 time: 0.0770\n",
      "Epoch 296, train_loss: 11.2424, train_acc: 0.9134, time: 0.2270\n",
      "\n",
      "Epoch 297, train_loss: 11.3472, train_acc: 0.9351, time: 0.2264\n",
      "\n",
      "Epoch 298, train_loss: 11.2271, train_acc: 0.9307, time: 0.2265\n",
      "\n",
      "Epoch 299, train_loss: 11.2051, train_acc: 0.9177, time: 0.2260\n",
      "\n",
      "Epoch 300, train_loss: 11.2760, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1764  acc: 0.4597  f1: 0.4662\n",
      "Test  loss: 2.0925 acc: 0.5293 f1: 0.2975 time: 0.0768\n",
      "VALID: VALID ACC 0.49289101362228394  VALID F1 0.4969021133642839 EPOCH 150\n",
      "VALID: TEST ACC 0.5087884664535522 TEST F1 0.2956570477202706 EPOCH 150\n",
      "GLOBAL: TEST ACC 0.5319104194641113 TEST F1 0.29918939618592644 EPOCH 250\n",
      "Epoch 301, train_loss: 11.2200, train_acc: 0.9177, time: 0.2264\n",
      "\n",
      "Epoch 302, train_loss: 11.2363, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Epoch 303, train_loss: 11.2217, train_acc: 0.9264, time: 0.2266\n",
      "\n",
      "Epoch 304, train_loss: 11.2733, train_acc: 0.9221, time: 0.2268\n",
      "\n",
      "Epoch 305, train_loss: 11.2045, train_acc: 0.9264, time: 0.2260\n",
      "\n",
      "Valid  loss: 2.1819  acc: 0.4502  f1: 0.4529\n",
      "Test  loss: 2.1220 acc: 0.5193 f1: 0.2863 time: 0.0765\n",
      "Epoch 306, train_loss: 11.2048, train_acc: 0.9177, time: 0.2268\n",
      "\n",
      "Epoch 307, train_loss: 11.1972, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 308, train_loss: 11.1765, train_acc: 0.9221, time: 0.2263\n",
      "\n",
      "Epoch 309, train_loss: 11.2330, train_acc: 0.9351, time: 0.2258\n",
      "\n",
      "Epoch 310, train_loss: 11.1916, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1793  acc: 0.4645  f1: 0.4680\n",
      "Test  loss: 2.1153 acc: 0.5157 f1: 0.2869 time: 0.0766\n",
      "Epoch 311, train_loss: 11.2018, train_acc: 0.9221, time: 0.2266\n",
      "\n",
      "Epoch 312, train_loss: 11.2741, train_acc: 0.9437, time: 0.2265\n",
      "\n",
      "Epoch 313, train_loss: 11.2129, train_acc: 0.9221, time: 0.2267\n",
      "\n",
      "Epoch 314, train_loss: 11.2307, train_acc: 0.9394, time: 0.2269\n",
      "\n",
      "Epoch 315, train_loss: 11.2387, train_acc: 0.9307, time: 0.2264\n",
      "\n",
      "Valid  loss: 2.1691  acc: 0.4834  f1: 0.4808\n",
      "Test  loss: 2.0894 acc: 0.5391 f1: 0.2992 time: 0.0767\n",
      "Epoch 316, train_loss: 11.2768, train_acc: 0.9177, time: 0.2268\n",
      "\n",
      "Epoch 317, train_loss: 11.2362, train_acc: 0.9134, time: 0.2268\n",
      "\n",
      "Epoch 318, train_loss: 11.2092, train_acc: 0.9437, time: 0.2270\n",
      "\n",
      "Epoch 319, train_loss: 11.2170, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 320, train_loss: 11.2578, train_acc: 0.9177, time: 0.2259\n",
      "\n",
      "Valid  loss: 2.1698  acc: 0.4645  f1: 0.4630\n",
      "Test  loss: 2.0922 acc: 0.5297 f1: 0.2899 time: 0.0766\n",
      "Epoch 321, train_loss: 11.2165, train_acc: 0.9307, time: 0.2264\n",
      "\n",
      "Epoch 322, train_loss: 11.2292, train_acc: 0.9264, time: 0.2263\n",
      "\n",
      "Epoch 323, train_loss: 11.2060, train_acc: 0.9307, time: 0.2268\n",
      "\n",
      "Epoch 324, train_loss: 11.1780, train_acc: 0.9307, time: 0.2266\n",
      "\n",
      "Epoch 325, train_loss: 11.2125, train_acc: 0.9264, time: 0.2264\n",
      "\n",
      "Valid  loss: 2.1689  acc: 0.4597  f1: 0.4615\n",
      "Test  loss: 2.1016 acc: 0.5258 f1: 0.2923 time: 0.0766\n",
      "Epoch 326, train_loss: 11.2085, train_acc: 0.9351, time: 0.2266\n",
      "\n",
      "Epoch 327, train_loss: 11.1860, train_acc: 0.9351, time: 0.2265\n",
      "\n",
      "Epoch 328, train_loss: 11.1779, train_acc: 0.9307, time: 0.2269\n",
      "\n",
      "Epoch 329, train_loss: 11.2302, train_acc: 0.9307, time: 0.2258\n",
      "\n",
      "Epoch 330, train_loss: 11.1664, train_acc: 0.9351, time: 0.2260\n",
      "\n",
      "Valid  loss: 2.1694  acc: 0.4787  f1: 0.4747\n",
      "Test  loss: 2.1058 acc: 0.5203 f1: 0.2834 time: 0.0766\n",
      "Epoch 331, train_loss: 11.2004, train_acc: 0.9048, time: 0.2261\n",
      "\n",
      "Epoch 332, train_loss: 11.2162, train_acc: 0.9307, time: 0.2265\n",
      "\n",
      "Epoch 333, train_loss: 11.2078, train_acc: 0.9351, time: 0.2260\n",
      "\n",
      "Epoch 334, train_loss: 11.1884, train_acc: 0.9481, time: 0.2262\n",
      "\n",
      "Epoch 335, train_loss: 11.2459, train_acc: 0.9177, time: 0.2267\n",
      "\n",
      "Valid  loss: 2.1713  acc: 0.4550  f1: 0.4220\n",
      "Test  loss: 2.0944 acc: 0.5335 f1: 0.3002 time: 0.0763\n",
      "Epoch 336, train_loss: 11.1925, train_acc: 0.9177, time: 0.2262\n",
      "\n",
      "Epoch 337, train_loss: 11.1788, train_acc: 0.9307, time: 0.2267\n",
      "\n",
      "Epoch 338, train_loss: 11.1829, train_acc: 0.9351, time: 0.2263\n",
      "\n",
      "Epoch 339, train_loss: 11.1672, train_acc: 0.9351, time: 0.2262\n",
      "\n",
      "Epoch 340, train_loss: 11.1740, train_acc: 0.9307, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1639  acc: 0.4645  f1: 0.4694\n",
      "Test  loss: 2.0887 acc: 0.5381 f1: 0.2955 time: 0.0771\n",
      "Epoch 341, train_loss: 11.1933, train_acc: 0.9177, time: 0.2270\n",
      "\n",
      "Epoch 342, train_loss: 11.1901, train_acc: 0.9394, time: 0.2265\n",
      "\n",
      "Epoch 343, train_loss: 11.1973, train_acc: 0.9351, time: 0.2259\n",
      "\n",
      "Epoch 344, train_loss: 11.1996, train_acc: 0.9221, time: 0.2261\n",
      "\n",
      "Epoch 345, train_loss: 11.1619, train_acc: 0.9264, time: 0.2264\n",
      "\n",
      "Valid  loss: 2.1766  acc: 0.4408  f1: 0.4400\n",
      "Test  loss: 2.0969 acc: 0.5437 f1: 0.2919 time: 0.0763\n",
      "Epoch 346, train_loss: 11.1998, train_acc: 0.9221, time: 0.2262\n",
      "\n",
      "Epoch 347, train_loss: 11.2766, train_acc: 0.9351, time: 0.2267\n",
      "\n",
      "Epoch 348, train_loss: 11.1477, train_acc: 0.9351, time: 0.2270\n",
      "\n",
      "Epoch 349, train_loss: 11.2131, train_acc: 0.9307, time: 0.2271\n",
      "\n",
      "Epoch 350, train_loss: 11.2295, train_acc: 0.9307, time: 0.2265\n",
      "\n",
      "Valid  loss: 2.1736  acc: 0.4692  f1: 0.4742\n",
      "Test  loss: 2.1057 acc: 0.5304 f1: 0.2988 time: 0.0764\n",
      "Epoch 351, train_loss: 11.1768, train_acc: 0.9307, time: 0.2262\n",
      "\n",
      "Epoch 352, train_loss: 11.2395, train_acc: 0.9177, time: 0.2265\n",
      "\n",
      "Epoch 353, train_loss: 11.2088, train_acc: 0.9134, time: 0.2259\n",
      "\n",
      "Epoch 354, train_loss: 11.2702, train_acc: 0.9307, time: 0.2262\n",
      "\n",
      "Epoch 355, train_loss: 11.1844, train_acc: 0.9134, time: 0.2266\n",
      "\n",
      "Valid  loss: 2.1884  acc: 0.4787  f1: 0.4733\n",
      "Test  loss: 2.1125 acc: 0.5219 f1: 0.2806 time: 0.0765\n",
      "Epoch 356, train_loss: 11.1973, train_acc: 0.9134, time: 0.2263\n",
      "\n",
      "Epoch 357, train_loss: 11.1996, train_acc: 0.9221, time: 0.2268\n",
      "\n",
      "Epoch 358, train_loss: 11.1549, train_acc: 0.9351, time: 0.2271\n",
      "\n",
      "Epoch 359, train_loss: 11.3141, train_acc: 0.9264, time: 0.2268\n",
      "\n",
      "Epoch 360, train_loss: 11.2433, train_acc: 0.9264, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1608  acc: 0.4597  f1: 0.4602\n",
      "Test  loss: 2.0794 acc: 0.5517 f1: 0.2979 time: 0.0769\n",
      "Epoch 361, train_loss: 11.2608, train_acc: 0.9177, time: 0.2268\n",
      "\n",
      "Epoch 362, train_loss: 11.1920, train_acc: 0.9221, time: 0.2271\n",
      "\n",
      "Epoch 363, train_loss: 11.2249, train_acc: 0.9221, time: 0.2263\n",
      "\n",
      "Epoch 364, train_loss: 11.2410, train_acc: 0.9221, time: 0.2271\n",
      "\n",
      "Epoch 365, train_loss: 11.1893, train_acc: 0.9394, time: 0.2265\n",
      "\n",
      "Valid  loss: 2.1758  acc: 0.4597  f1: 0.4221\n",
      "Test  loss: 2.0863 acc: 0.5482 f1: 0.2954 time: 0.0763\n",
      "Epoch 366, train_loss: 11.1676, train_acc: 0.9437, time: 0.2268\n",
      "\n",
      "Epoch 367, train_loss: 11.2660, train_acc: 0.9177, time: 0.2266\n",
      "\n",
      "Epoch 368, train_loss: 11.2433, train_acc: 0.9221, time: 0.2270\n",
      "\n",
      "Epoch 369, train_loss: 11.2423, train_acc: 0.9264, time: 0.2260\n",
      "\n",
      "Epoch 370, train_loss: 11.2024, train_acc: 0.9351, time: 0.2259\n",
      "\n",
      "Valid  loss: 2.1705  acc: 0.4645  f1: 0.4705\n",
      "Test  loss: 2.0820 acc: 0.5504 f1: 0.2966 time: 0.0767\n",
      "Epoch 371, train_loss: 11.2491, train_acc: 0.9264, time: 0.2261\n",
      "\n",
      "Epoch 372, train_loss: 11.2093, train_acc: 0.9394, time: 0.2264\n",
      "\n",
      "Epoch 373, train_loss: 11.2161, train_acc: 0.9351, time: 0.2258\n",
      "\n",
      "Epoch 374, train_loss: 11.1985, train_acc: 0.9264, time: 0.2263\n",
      "\n",
      "Epoch 375, train_loss: 11.2079, train_acc: 0.9221, time: 0.2267\n",
      "\n",
      "Valid  loss: 2.1793  acc: 0.4550  f1: 0.4181\n",
      "Test  loss: 2.0798 acc: 0.5610 f1: 0.2983 time: 0.0761\n",
      "Epoch 376, train_loss: 11.2130, train_acc: 0.9221, time: 0.2261\n",
      "\n",
      "Epoch 377, train_loss: 11.2315, train_acc: 0.9221, time: 0.2268\n",
      "\n",
      "Epoch 378, train_loss: 11.2010, train_acc: 0.9351, time: 0.2270\n",
      "\n",
      "Epoch 379, train_loss: 11.2202, train_acc: 0.9307, time: 0.2264\n",
      "\n",
      "Epoch 380, train_loss: 11.1708, train_acc: 0.9351, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1650  acc: 0.4645  f1: 0.4670\n",
      "Test  loss: 2.0852 acc: 0.5472 f1: 0.2958 time: 0.0770\n",
      "Epoch 381, train_loss: 11.2371, train_acc: 0.9437, time: 0.2267\n",
      "\n",
      "Epoch 382, train_loss: 11.1481, train_acc: 0.9394, time: 0.2271\n",
      "\n",
      "Epoch 383, train_loss: 11.2071, train_acc: 0.9264, time: 0.2259\n",
      "\n",
      "Epoch 384, train_loss: 11.1957, train_acc: 0.9177, time: 0.2262\n",
      "\n",
      "Epoch 385, train_loss: 11.2536, train_acc: 0.9264, time: 0.2265\n",
      "\n",
      "Valid  loss: 2.1692  acc: 0.4550  f1: 0.4594\n",
      "Test  loss: 2.0824 acc: 0.5574 f1: 0.2950 time: 0.0761\n",
      "Epoch 386, train_loss: 11.1588, train_acc: 0.9264, time: 0.2264\n",
      "\n",
      "Epoch 387, train_loss: 11.2839, train_acc: 0.9177, time: 0.2268\n",
      "\n",
      "Epoch 388, train_loss: 11.1636, train_acc: 0.9221, time: 0.2264\n",
      "\n",
      "Epoch 389, train_loss: 11.1770, train_acc: 0.9221, time: 0.2261\n",
      "\n",
      "Epoch 390, train_loss: 11.2276, train_acc: 0.9351, time: 0.2267\n",
      "\n",
      "Valid  loss: 2.1751  acc: 0.4597  f1: 0.4642\n",
      "Test  loss: 2.0900 acc: 0.5447 f1: 0.2938 time: 0.0771\n",
      "Epoch 391, train_loss: 11.1929, train_acc: 0.9264, time: 0.2270\n",
      "\n",
      "Epoch 392, train_loss: 11.2876, train_acc: 0.9091, time: 0.2267\n",
      "\n",
      "Epoch 393, train_loss: 11.1948, train_acc: 0.9221, time: 0.2266\n",
      "\n",
      "Epoch 394, train_loss: 11.1988, train_acc: 0.9264, time: 0.2265\n",
      "\n",
      "Epoch 395, train_loss: 11.1722, train_acc: 0.9134, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1782  acc: 0.4550  f1: 0.4508\n",
      "Test  loss: 2.1034 acc: 0.5312 f1: 0.2843 time: 0.0762\n",
      "Epoch 396, train_loss: 11.2134, train_acc: 0.9221, time: 0.2269\n",
      "\n",
      "Epoch 397, train_loss: 11.2573, train_acc: 0.9221, time: 0.2263\n",
      "\n",
      "Epoch 398, train_loss: 11.2335, train_acc: 0.9221, time: 0.2265\n",
      "\n",
      "Epoch 399, train_loss: 11.2073, train_acc: 0.9221, time: 0.2264\n",
      "\n",
      "Epoch 400, train_loss: 11.2227, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1784  acc: 0.4408  f1: 0.4470\n",
      "Test  loss: 2.0988 acc: 0.5343 f1: 0.2830 time: 0.0766\n",
      "VALID: VALID ACC 0.49289101362228394  VALID F1 0.4969021133642839 EPOCH 150\n",
      "VALID: TEST ACC 0.5087884664535522 TEST F1 0.2956570477202706 EPOCH 150\n",
      "GLOBAL: TEST ACC 0.5609960556030273 TEST F1 0.298316872281244 EPOCH 375\n",
      "Epoch 401, train_loss: 11.2407, train_acc: 0.9307, time: 0.2262\n",
      "\n",
      "Epoch 402, train_loss: 11.1830, train_acc: 0.9134, time: 0.2266\n",
      "\n",
      "Epoch 403, train_loss: 11.1779, train_acc: 0.9091, time: 0.2260\n",
      "\n",
      "Epoch 404, train_loss: 11.2446, train_acc: 0.9437, time: 0.2264\n",
      "\n",
      "Epoch 405, train_loss: 11.1549, train_acc: 0.9351, time: 0.2268\n",
      "\n",
      "Valid  loss: 2.1600  acc: 0.4550  f1: 0.4226\n",
      "Test  loss: 2.0946 acc: 0.5423 f1: 0.3019 time: 0.0764\n",
      "Epoch 406, train_loss: 11.2695, train_acc: 0.9221, time: 0.2270\n",
      "\n",
      "Epoch 407, train_loss: 11.1925, train_acc: 0.9351, time: 0.2269\n",
      "\n",
      "Epoch 408, train_loss: 11.1662, train_acc: 0.9221, time: 0.2273\n",
      "\n",
      "Epoch 409, train_loss: 11.1547, train_acc: 0.9307, time: 0.2274\n",
      "\n",
      "Epoch 410, train_loss: 11.1866, train_acc: 0.9264, time: 0.2265\n",
      "\n",
      "Valid  loss: 2.1710  acc: 0.4597  f1: 0.4271\n",
      "Test  loss: 2.0973 acc: 0.5378 f1: 0.2934 time: 0.0767\n",
      "Epoch 411, train_loss: 11.1956, train_acc: 0.9134, time: 0.2266\n",
      "\n",
      "Epoch 412, train_loss: 11.1210, train_acc: 0.9351, time: 0.2266\n",
      "\n",
      "Epoch 413, train_loss: 11.1588, train_acc: 0.9221, time: 0.2269\n",
      "\n",
      "Epoch 414, train_loss: 11.1690, train_acc: 0.9221, time: 0.2269\n",
      "\n",
      "Epoch 415, train_loss: 11.2163, train_acc: 0.9351, time: 0.2261\n",
      "\n",
      "Valid  loss: 2.1724  acc: 0.4739  f1: 0.4733\n",
      "Test  loss: 2.0964 acc: 0.5427 f1: 0.2925 time: 0.0766\n",
      "Epoch 416, train_loss: 11.1875, train_acc: 0.9264, time: 0.2264\n",
      "\n",
      "Epoch 417, train_loss: 11.2152, train_acc: 0.9394, time: 0.2262\n",
      "\n",
      "Epoch 418, train_loss: 11.1458, train_acc: 0.9394, time: 0.2266\n",
      "\n",
      "Epoch 419, train_loss: 11.1754, train_acc: 0.9394, time: 0.2260\n",
      "\n",
      "Epoch 420, train_loss: 11.2896, train_acc: 0.9351, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1727  acc: 0.4692  f1: 0.4680\n",
      "Test  loss: 2.0823 acc: 0.5472 f1: 0.2929 time: 0.0766\n",
      "Epoch 421, train_loss: 11.1802, train_acc: 0.9307, time: 0.2264\n",
      "\n",
      "Epoch 422, train_loss: 11.1937, train_acc: 0.9394, time: 0.2263\n",
      "\n",
      "Epoch 423, train_loss: 11.1601, train_acc: 0.9307, time: 0.2268\n",
      "\n",
      "Epoch 424, train_loss: 11.1655, train_acc: 0.9351, time: 0.2264\n",
      "\n",
      "Epoch 425, train_loss: 11.2328, train_acc: 0.9307, time: 0.2266\n",
      "\n",
      "Valid  loss: 2.1685  acc: 0.4597  f1: 0.4606\n",
      "Test  loss: 2.0922 acc: 0.5436 f1: 0.2899 time: 0.0764\n",
      "Epoch 426, train_loss: 11.2017, train_acc: 0.9437, time: 0.2268\n",
      "\n",
      "Epoch 427, train_loss: 11.2118, train_acc: 0.9264, time: 0.2267\n",
      "\n",
      "Epoch 428, train_loss: 11.1740, train_acc: 0.9481, time: 0.2268\n",
      "\n",
      "Epoch 429, train_loss: 11.1546, train_acc: 0.9307, time: 0.2260\n",
      "\n",
      "Epoch 430, train_loss: 11.1916, train_acc: 0.9394, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1632  acc: 0.4834  f1: 0.4839\n",
      "Test  loss: 2.0818 acc: 0.5517 f1: 0.2915 time: 0.0771\n",
      "Epoch 431, train_loss: 11.1809, train_acc: 0.9351, time: 0.2271\n",
      "\n",
      "Epoch 432, train_loss: 11.1558, train_acc: 0.9351, time: 0.2268\n",
      "\n",
      "Epoch 433, train_loss: 11.2420, train_acc: 0.9351, time: 0.2271\n",
      "\n",
      "Epoch 434, train_loss: 11.1290, train_acc: 0.9307, time: 0.2273\n",
      "\n",
      "Epoch 435, train_loss: 11.1162, train_acc: 0.9264, time: 0.2269\n",
      "\n",
      "Valid  loss: 2.1686  acc: 0.4455  f1: 0.4500\n",
      "Test  loss: 2.0816 acc: 0.5605 f1: 0.3009 time: 0.0770\n",
      "Epoch 436, train_loss: 11.1393, train_acc: 0.9264, time: 0.2275\n",
      "\n",
      "Epoch 437, train_loss: 11.1726, train_acc: 0.9221, time: 0.2272\n",
      "\n",
      "Epoch 438, train_loss: 11.1540, train_acc: 0.9307, time: 0.2261\n",
      "\n",
      "Epoch 439, train_loss: 11.1537, train_acc: 0.9351, time: 0.2265\n",
      "\n",
      "Epoch 440, train_loss: 11.2283, train_acc: 0.9134, time: 0.2270\n",
      "\n",
      "Valid  loss: 2.1784  acc: 0.4502  f1: 0.4084\n",
      "Test  loss: 2.0683 acc: 0.5756 f1: 0.3010 time: 0.0766\n",
      "Epoch 441, train_loss: 11.1841, train_acc: 0.9351, time: 0.2263\n",
      "\n",
      "Epoch 442, train_loss: 11.1708, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Epoch 443, train_loss: 11.1380, train_acc: 0.9264, time: 0.2265\n",
      "\n",
      "Epoch 444, train_loss: 11.1275, train_acc: 0.9264, time: 0.2267\n",
      "\n",
      "Epoch 445, train_loss: 11.2101, train_acc: 0.9091, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1829  acc: 0.4502  f1: 0.4499\n",
      "Test  loss: 2.0898 acc: 0.5477 f1: 0.2859 time: 0.0766\n",
      "Epoch 446, train_loss: 11.2316, train_acc: 0.8918, time: 0.2268\n",
      "\n",
      "Epoch 447, train_loss: 11.1872, train_acc: 0.9307, time: 0.2265\n",
      "\n",
      "Epoch 448, train_loss: 11.1704, train_acc: 0.9394, time: 0.2263\n",
      "\n",
      "Epoch 449, train_loss: 11.1673, train_acc: 0.9394, time: 0.2268\n",
      "\n",
      "Epoch 450, train_loss: 11.1461, train_acc: 0.9307, time: 0.2269\n",
      "\n",
      "Valid  loss: 2.1732  acc: 0.4597  f1: 0.4230\n",
      "Test  loss: 2.0809 acc: 0.5628 f1: 0.3057 time: 0.0772\n",
      "Epoch 451, train_loss: 11.1909, train_acc: 0.9394, time: 0.2266\n",
      "\n",
      "Epoch 452, train_loss: 11.1577, train_acc: 0.9307, time: 0.2268\n",
      "\n",
      "Epoch 453, train_loss: 11.2300, train_acc: 0.9264, time: 0.2271\n",
      "\n",
      "Epoch 454, train_loss: 11.1946, train_acc: 0.9351, time: 0.2270\n",
      "\n",
      "Epoch 455, train_loss: 11.1084, train_acc: 0.9351, time: 0.2267\n",
      "\n",
      "Valid  loss: 2.1826  acc: 0.4360  f1: 0.4438\n",
      "Test  loss: 2.0944 acc: 0.5508 f1: 0.2932 time: 0.0768\n",
      "Epoch 456, train_loss: 11.1477, train_acc: 0.9394, time: 0.2268\n",
      "\n",
      "Epoch 457, train_loss: 11.1968, train_acc: 0.9307, time: 0.2272\n",
      "\n",
      "Epoch 458, train_loss: 11.1631, train_acc: 0.9307, time: 0.2268\n",
      "\n",
      "Epoch 459, train_loss: 11.1861, train_acc: 0.9437, time: 0.2270\n",
      "\n",
      "Epoch 460, train_loss: 11.2453, train_acc: 0.9394, time: 0.2264\n",
      "\n",
      "Valid  loss: 2.1652  acc: 0.4597  f1: 0.4665\n",
      "Test  loss: 2.0677 acc: 0.5739 f1: 0.3028 time: 0.0767\n",
      "Epoch 461, train_loss: 11.1851, train_acc: 0.9394, time: 0.2269\n",
      "\n",
      "Epoch 462, train_loss: 11.2517, train_acc: 0.9264, time: 0.2268\n",
      "\n",
      "Epoch 463, train_loss: 11.1598, train_acc: 0.9351, time: 0.2271\n",
      "\n",
      "Epoch 464, train_loss: 11.1736, train_acc: 0.9307, time: 0.2260\n",
      "\n",
      "Epoch 465, train_loss: 11.1310, train_acc: 0.9221, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1789  acc: 0.4645  f1: 0.4283\n",
      "Test  loss: 2.0742 acc: 0.5755 f1: 0.2995 time: 0.0766\n",
      "Epoch 466, train_loss: 11.1166, train_acc: 0.9307, time: 0.2263\n",
      "\n",
      "Epoch 467, train_loss: 11.1778, train_acc: 0.9394, time: 0.2265\n",
      "\n",
      "Epoch 468, train_loss: 11.1300, train_acc: 0.9394, time: 0.2269\n",
      "\n",
      "Epoch 469, train_loss: 11.2482, train_acc: 0.9307, time: 0.2265\n",
      "\n",
      "Epoch 470, train_loss: 11.2240, train_acc: 0.9307, time: 0.2267\n",
      "\n",
      "Valid  loss: 2.1544  acc: 0.4645  f1: 0.4667\n",
      "Test  loss: 2.0821 acc: 0.5662 f1: 0.2992 time: 0.0765\n",
      "Epoch 471, train_loss: 11.1505, train_acc: 0.9351, time: 0.2268\n",
      "\n",
      "Epoch 472, train_loss: 11.1437, train_acc: 0.9221, time: 0.2268\n",
      "\n",
      "Epoch 473, train_loss: 11.2364, train_acc: 0.9351, time: 0.2274\n",
      "\n",
      "Epoch 474, train_loss: 11.1147, train_acc: 0.9221, time: 0.2259\n",
      "\n",
      "Epoch 475, train_loss: 11.1856, train_acc: 0.9481, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1769  acc: 0.4692  f1: 0.4333\n",
      "Test  loss: 2.0760 acc: 0.5762 f1: 0.3046 time: 0.0769\n",
      "Epoch 476, train_loss: 11.1378, train_acc: 0.9394, time: 0.2263\n",
      "\n",
      "Epoch 477, train_loss: 11.1424, train_acc: 0.9351, time: 0.2263\n",
      "\n",
      "Epoch 478, train_loss: 11.1732, train_acc: 0.9437, time: 0.2268\n",
      "\n",
      "Epoch 479, train_loss: 11.1866, train_acc: 0.9307, time: 0.2274\n",
      "\n",
      "Epoch 480, train_loss: 11.1654, train_acc: 0.9264, time: 0.2267\n",
      "\n",
      "Valid  loss: 2.1784  acc: 0.4455  f1: 0.4494\n",
      "Test  loss: 2.0752 acc: 0.5745 f1: 0.2975 time: 0.0765\n",
      "Epoch 481, train_loss: 11.2571, train_acc: 0.9351, time: 0.2271\n",
      "\n",
      "Epoch 482, train_loss: 11.1428, train_acc: 0.9394, time: 0.2270\n",
      "\n",
      "Epoch 483, train_loss: 11.1377, train_acc: 0.9264, time: 0.2263\n",
      "\n",
      "Epoch 484, train_loss: 11.1920, train_acc: 0.9437, time: 0.2266\n",
      "\n",
      "Epoch 485, train_loss: 11.1678, train_acc: 0.9351, time: 0.2269\n",
      "\n",
      "Valid  loss: 2.1816  acc: 0.4502  f1: 0.4561\n",
      "Test  loss: 2.0775 acc: 0.5682 f1: 0.3022 time: 0.0767\n",
      "Epoch 486, train_loss: 11.1809, train_acc: 0.9437, time: 0.2264\n",
      "\n",
      "Epoch 487, train_loss: 11.1951, train_acc: 0.9307, time: 0.2263\n",
      "\n",
      "Epoch 488, train_loss: 11.1576, train_acc: 0.9351, time: 0.2261\n",
      "\n",
      "Epoch 489, train_loss: 11.1460, train_acc: 0.9221, time: 0.2265\n",
      "\n",
      "Epoch 490, train_loss: 11.1299, train_acc: 0.9264, time: 0.2262\n",
      "\n",
      "Valid  loss: 2.1717  acc: 0.4502  f1: 0.4457\n",
      "Test  loss: 2.0815 acc: 0.5640 f1: 0.2942 time: 0.0767\n",
      "Epoch 491, train_loss: 11.1627, train_acc: 0.9048, time: 0.2269\n",
      "\n",
      "Epoch 492, train_loss: 11.1567, train_acc: 0.9134, time: 0.2274\n",
      "\n",
      "Epoch 493, train_loss: 11.1628, train_acc: 0.9307, time: 0.2267\n",
      "\n",
      "Epoch 494, train_loss: 11.1317, train_acc: 0.9221, time: 0.2271\n",
      "\n",
      "Epoch 495, train_loss: 11.1575, train_acc: 0.9307, time: 0.2275\n",
      "\n",
      "Valid  loss: 2.1604  acc: 0.4739  f1: 0.4720\n",
      "Test  loss: 2.0790 acc: 0.5619 f1: 0.2939 time: 0.0771\n",
      "Epoch 496, train_loss: 11.1734, train_acc: 0.9307, time: 0.2269\n",
      "\n",
      "Epoch 497, train_loss: 11.2179, train_acc: 0.9221, time: 0.2276\n",
      "\n",
      "Epoch 498, train_loss: 11.1924, train_acc: 0.9221, time: 0.2274\n",
      "\n",
      "Epoch 499, train_loss: 11.0981, train_acc: 0.9307, time: 0.2273\n",
      "\n",
      "Epoch 500, train_loss: 11.1267, train_acc: 0.9221, time: 0.2261\n",
      "\n",
      "Valid  loss: 2.1771  acc: 0.4550  f1: 0.4569\n",
      "Test  loss: 2.0750 acc: 0.5790 f1: 0.2990 time: 0.0767\n",
      "VALID: VALID ACC 0.49289101362228394  VALID F1 0.4969021133642839 EPOCH 150\n",
      "VALID: TEST ACC 0.5087884664535522 TEST F1 0.2956570477202706 EPOCH 150\n",
      "GLOBAL: TEST ACC 0.5789914131164551 TEST F1 0.29904117907347355 EPOCH 500\n"
     ]
    }
   ],
   "source": [
    "with open('/home/bighillanal/SimSTCFork/train_log.md', 'w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "!python train.py --dataset jail_breaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47beefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taehun_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
